[
["index.html", "R Data Science Book 1 Introduction 1.1 Goals 1.2 Data 1.3 Git and GitHub", " R Data Science Book Tools and Programming Languages for Data Scientists, FH Kiel, Summer Term 2020 2020-05-14 1 Introduction 1.1 Goals This book is a joint effort of the course “Tools and Programming Languages for Data Science”, FH Kiel. We develop this book, in order to learn how typical data wrangling tasks can be solved using the programming language R and in order to practice collaborative programming workflows using Git and GitHub. All of the R packages that we are going to cover are already extensively documented in books, online, and R help files. Our mission for this book is to investigate what these packages are good for, think about good example use cases in the context of data that we know, and apply their functionalities to these data. We start with core tidyverse packages that facilitate the data science workflow: tidying the data using the tidyr package, and exploring the data using the dplyr package. We work on tidyverse packages dedicated for specific data types: stringr for text data, lubridate for dates and times, and forcats for categorical variables. If data sets are huge we may run into performance problems. Hence, we explore the advantages of the data.table package for high performance computing in R. 1.2 Data The whole book is supposed to be based on the data contained in the data subdirectory. Please ask me if you would like to add another data set (e.g. because it would allow you to better demonstrate the functionalities of your package). Currently, the following data sets are covered: diamonds: data on diamonds; 50000 rows; categorical and numeric variables spotify-charts-germany: German daily top 200 charts for one year; 70000 rows; mostly numeric variables and dates olympic-games: data on olympic games medallists; 250000 rows, categorical and numeric data recipes: data on recipes; 60000 rows; character, date, and numeric variables related information weather-kiel-holtenau: weather data for Kiel-Holtenau in 10-Minute intervals for one year, 50000 rows; Date, time and numeric variables. 1.3 Git and GitHub We will work in teams of 2 students per topic. An important part of the book project is practicing collaborative workflows using Git and Github. We will use the Forking Workflow which is typical of open source projects. This involves the following steps: Fork the ‘official’ GitHub repository (“upstream”). This creates your own GitHub copy (“origin”). Clone your fork to your local system. Connect your local clone to the upstream repo by adding a remote path. Create a new local feature branch. Make changes on the new branch and create commits for these changes. Push the branch to your GitHub repo (“origin”) Open a pull request on GitHub to the upstream repository. Your team mate reviews your pull request. Once approved, it is merged into the upstream repo. How to connect your local clone to the upstream repo? # Check the currently registered remote repositories git remote -v # Add the upstream repo git remote add upstream https://github.com/tillschwoerer/R-data-science-book.git How can I integrate changes in the upstream repo into my local system? Best practice is to regularly pull upstream changes into the master branch of your local system, and then create a new feature branch, in which you make your own edits and commits. Never edit the master yourself. If you follow this routine, the pull won’t cause any conflicts - it will be a so called fast forward merge. To be on the save side use git pull upstream master --ff-only. The --ff-only flag means that the upstream changes are merged into the master only if it is a fast forward merge. If you have accidently made commits to the master, you will get an error message. In this case follow the steps described here to resolve the conflict. Further literature: https://happygitwithr.com/fork-and-clone.html https://www.atlassian.com/git/tutorials/comparing-workflows/forking-workflow "],
["readr.html", "2 Reading data: readr", " 2 Reading data: readr library(readr) # reading data (we could also load it via package tidyverse) library(dplyr) # wrangling data (we could also load it via package tidyverse) We are working with crudly formatted contract data (vertragsdaten.csv), to demonstrate the functionalites of the readr package. This is how the data looks like: readr::read_lines(file = &quot;data/vertragsdaten.csv&quot;, n_max = 5) ## [1] &quot;0135710150010017;01;3;1;01; 156,50 ;22022020;H ;00 ;17D2;V709;&quot; ## [2] &quot;0136300200006011;01;3;1;01; 60,40 ;22022020;H ;00 ;17D2;V709;&quot; ## [3] &quot;0136920590002028;01;3;1;01; 1,30 ;22022020;H ;10 ;14H2;V113;&quot; ## [4] &quot;0171400040019017;01;3;2;01; 61,00 ;22022020;H ;00 ;17D2;V113;&quot; ## [5] &quot;0272150100001011;07;3;1;01; 9.842,50 ;22022020;H ;00 ; 662;V002;&quot; The Challenges are: Add column names Deal with leading and trailing blanks (which can have a meaning in some columns) Correctly specify the column types (numbers, date, character, …) To correctly identify numbers, we need to set the German locale. Itherwise, the decimal mark won’t be correctly identified. In order to create readible reports, it is sometimes helpful to display Euro amounts not as a number, but as a character that includes the Euro symbol (€) and comma/big mark signs df &lt;- readr::read_delim(file = &quot;data/vertragsdaten.csv&quot;, # Specify semicolon as separator delim = &quot;;&quot;, # Set the missing column names col_names = c(&quot;VVT_NR&quot;, &quot;Gebiet&quot;, &quot;Abt&quot;, &quot;Zw&quot;, &quot;FK&quot;, &quot;Beitr_Neu&quot;, &quot;EDV_Dat&quot;, &quot;Merkmal&quot;, &quot;Kz_MB&quot;, &quot;Sparte&quot;, &quot;Bedingung&quot;, &quot;leere_Spalte&quot;), # Change data types which are not correctly imported by default # We skip the last column, which turns out to be empty col_types = cols(EDV_Dat = col_date(format = &quot;%d%m%Y&quot;), leere_Spalte = col_skip()), # We set the German locale in order to get numbers and dates right locale = locale(date_names = &quot;de&quot;, decimal_mark = &quot;,&quot;, grouping_mark = &quot;.&quot;), # White spaces are sometimes important, so we don&#39;t want to trim them trim_ws = FALSE) head(df) ## # A tibble: 5 x 11 ## VVT_NR Gebiet Abt Zw FK Beitr_Neu EDV_Dat Merkmal Kz_MB Sparte ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01357~ 01 3 1 01 &quot; ~ 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 2 01363~ 01 3 1 01 &quot; ~ 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 3 01369~ 01 3 1 01 &quot; ~ 2020-02-22 &quot;H ~ &quot;10 &quot; &quot;14H2&quot; ## 4 01714~ 01 3 2 01 &quot; ~ 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 5 02721~ 07 3 1 01 &quot; ~ 2020-02-22 &quot;H ~ &quot;00 &quot; &quot; 662&quot; ## # ... with 1 more variable: Bedingung &lt;chr&gt; The column Beitr_Neu is not yet correctly recognized as numeric, due to the fact that we did not trim white spaces. Hence, we need to correct this column in a separate step. df &lt;- df %&gt;% mutate(Beitr_Neu = readr::parse_number(Beitr_Neu, # We trim White psaces trim_ws = TRUE, # Set the German number separator marks locale = locale(decimal_mark = &quot;,&quot;, grouping_mark = &quot;.&quot;), )) head(df) ## # A tibble: 5 x 11 ## VVT_NR Gebiet Abt Zw FK Beitr_Neu EDV_Dat Merkmal Kz_MB Sparte ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01357~ 01 3 1 01 156. 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 2 01363~ 01 3 1 01 60.4 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 3 01369~ 01 3 1 01 1.3 2020-02-22 &quot;H ~ &quot;10 &quot; &quot;14H2&quot; ## 4 01714~ 01 3 2 01 61 2020-02-22 &quot;H ~ &quot;00 &quot; &quot;17D2&quot; ## 5 02721~ 07 3 1 01 9842. 2020-02-22 &quot;H ~ &quot;00 &quot; &quot; 662&quot; ## # ... with 1 more variable: Bedingung &lt;chr&gt; "],
["tidyr.html", "3 Tidying data: tidyr", " 3 Tidying data: tidyr "],
["dplyr.html", "4 Wrangling data: dplyr 4.1 Two typial workflows 4.2 Manipulating rows 4.3 Manipulating columns 4.4 Scoped functions 4.5 Aggregate 4.6 Window functions 4.7 Combining tables 4.8 Database backend", " 4 Wrangling data: dplyr The R package dplry is described as a grammar of data manipulation. It provides commands for the most frequently used types of data transformations, either for the purpose of exploration, data cleaning, or augmenting data. The package is part of the core tidyverse, which means that we can load it either explictly via library(dplyr) or implictly via library(tidyverse). Note that in the following, all commands that are not dplyr commands are made explicit via &lt;package&gt;::&lt;command&gt;. To demonstrate the main features, we use data on Spotify’s daily top 200 charts in Germany in the course of one year. library(tidyverse) # alternatively use library(tidyverse) which covers dplyr + more df &lt;- readr::read_csv(&quot;data/spotify_charts_germany.csv&quot;) 4.1 Two typial workflows Before looking in detail into specific functions, let’s start with two typical workflows. We will note that dplyr works with the pipe (%&gt;%) such that multiple operations can be combined one after the other, without the need to create intermediate results. function names are quite expressive, basically telling us what they are doing. there is a strong analogy to SQL: due to this analogy it is even possible to run dplyr commands with a database backend (the package dbplyr needs to be installed) The first workflow returns an ordered list of the 5 tracks with the highest number of streams on a single day: df %&gt;% # data select(Streams, date, Artist, Track.Name) %&gt;% # select columns by name arrange(-Streams) %&gt;% # order rows by some variable in descending order slice(1:5) # select rows by position ## # A tibble: 5 x 4 ## Streams date Artist Track.Name ## &lt;dbl&gt; &lt;date&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1964217 2019-12-24 Mariah Carey All I Want for Christmas Is You ## 2 1939974 2019-12-24 Wham! Last Christmas ## 3 1788621 2019-06-21 Capital Bra Tilidin ## 4 1603796 2019-12-24 Chris Rea Driving Home for Christmas - 2019 Remaster ## 5 1538169 2019-06-22 Capital Bra Tilidin The second workflow returns the average number of streams per day of week since the beginning of the year 2020. For this operation, the day of week is derived from the date and added as an additional variable via the mutate function. df %&gt;% # data filter(date&gt;=&quot;2020-01-01&quot;) %&gt;% # select rows where condition evaluates to TRUE # create an additional variable mutate(day_of_week=lubridate::wday(date, label=TRUE, abbr=FALSE, week_start=1)) %&gt;% group_by(day_of_week) %&gt;% # group the data summarise(streams = mean(Streams)) # aggregate per group via functions such as mean, min, etc. ## # A tibble: 7 x 2 ## day_of_week streams ## &lt;ord&gt; &lt;dbl&gt; ## 1 Montag 125182. ## 2 Dienstag 125230. ## 3 Mittwoch 122318. ## 4 Donnerstag 125505. ## 5 Freitag 156173. ## 6 Samstag 141063. ## 7 Sonntag 112455. Note that in this particular case, we can write the code more concise by generating the new variable day_of_week inside the group_by function. df %&gt;% filter(date&gt;=&quot;2020-01-01&quot;) %&gt;% group_by(day_of_week=lubridate::wday(date, label=TRUE, abbr=FALSE, week_start=1)) %&gt;% summarise(streams = mean(Streams)) 4.2 Manipulating rows 4.2.1 Extract rows The filter function is the most frequently used function to extract a subset of rows. The command extracts all rows where the filter condition(s) evaluate to TRUE. The distinct function returns distinct rows by removing duplicates (either for the whole data or the specified variables). df %&gt;% filter(stringr::str_detect(Track.Name, &quot;Santa&quot;)) %&gt;% # extract rows where condition is TRUE distinct(Artist, Track.Name) # extract distinct combinations of the two variables ## # A tibble: 13 x 2 ## Artist Track.Name ## &lt;chr&gt; &lt;chr&gt; ## 1 Ariana Grande Santa Tell Me ## 2 Kylie Minogue Santa Baby ## 3 Sia Santa&#39;s Coming For Us ## 4 Michael Bublé Santa Claus Is Coming to Town ## 5 Bruce Springst~ Santa Claus Is Comin&#39; to Town - Live at C.W. Post College, G~ ## 6 Frank Sinatra Santa Claus Is Comin&#39; to Town ## 7 Eartha Kitt Santa Baby (with Henri René &amp; His Orchestra) ## 8 Robbie Williams Santa Baby (feat. Helene Fischer) ## 9 Gene Autry Here Comes Santa Claus (Right Down Santa Claus Lane) ## 10 Rod Stewart Santa Claus Is Coming To Town ## 11 Ariana Grande Santa Baby ## 12 The Jackson 5 Santa Claus Is Coming To Town ## 13 Mariah Carey Oh Santa! We can select rows by position via slice. If we want to display the first or last n rows, we can also use the base R functions head and tail. The functions top_n of top_fraq allow us to extract the specified number/fraction of rows, according to the ordering of a specified variable. In addition, top_n and top_frac also operate on grouped data. df %&gt;% slice(c(1,3,5)) # selects rows by position (in the given order) ## # A tibble: 3 x 15 ## Position Track.Name Artist Streams date danceability energy loudness ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Cherry La~ Capit~ 1040382 2019-03-30 0.838 0.549 -7.14 ## 2 3 Blackberr~ Eno 704316 2019-03-30 0.805 0.625 -8.59 ## 3 5 Puerto Ri~ Fero47 557781 2019-03-30 0.687 0.766 -6.74 ## # ... with 7 more variables: speechiness &lt;dbl&gt;, acousticness &lt;dbl&gt;, ## # instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;, tempo &lt;dbl&gt;, ## # duration_ms &lt;dbl&gt; df %&gt;% head(3) # selects first n rows (in the given order) ## # A tibble: 3 x 15 ## Position Track.Name Artist Streams date danceability energy loudness ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Cherry La~ Capit~ 1040382 2019-03-30 0.838 0.549 -7.14 ## 2 2 Affalterb~ Shindy 822209 2019-03-30 0.819 0.674 -4.66 ## 3 3 Blackberr~ Eno 704316 2019-03-30 0.805 0.625 -8.59 ## # ... with 7 more variables: speechiness &lt;dbl&gt;, acousticness &lt;dbl&gt;, ## # instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;, tempo &lt;dbl&gt;, ## # duration_ms &lt;dbl&gt; df %&gt;% top_n(3, Streams) # selects top n rows (based on the variable Streams) ## # A tibble: 3 x 15 ## Position Track.Name Artist Streams date danceability energy loudness ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 Tilidin Capit~ 1788621 2019-06-21 0.631 0.673 -4.89 ## 2 1 All I Wan~ Maria~ 1964217 2019-12-24 0.335 0.625 -7.46 ## 3 2 Last Chri~ Wham! 1939974 2019-12-24 0.735 0.478 -12.5 ## # ... with 7 more variables: speechiness &lt;dbl&gt;, acousticness &lt;dbl&gt;, ## # instrumentalness &lt;dbl&gt;, liveness &lt;dbl&gt;, valence &lt;dbl&gt;, tempo &lt;dbl&gt;, ## # duration_ms &lt;dbl&gt; df %&gt;% group_by(date) %&gt;% # group by date top_n(1, wt=Streams) %&gt;% # select 1 row per date, the one with the highest number of streams select(date, Streams, Track.Name, Artist) %&gt;% head(5) ## # A tibble: 5 x 4 ## # Groups: date [5] ## date Streams Track.Name Artist ## &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 2019-03-30 1040382 Cherry Lady Capital Bra ## 2 2019-03-31 771685 Cherry Lady Capital Bra ## 3 2019-04-01 861671 Cherry Lady Capital Bra ## 4 2019-04-02 818911 Cherry Lady Capital Bra ## 5 2019-04-03 783832 Cherry Lady Capital Bra Another useful feature is selecting rows randomly via sample_n or sample_frac (output hidden). df %&gt;% sample_n(5) # Select 5 rows randomly with replacement df %&gt;% sample_frac(0.1, replace=TRUE) # Select a 10% random sample with replacement 4.2.2 Arranging rows The function arrange is used to order rows by some variable(s). Use minus (-) or the desc function for arranging in descending order. The following code returns the five most danceable chart tracks of 2019-03-30 by arranging first by date (ascending) and second by danceability (descending). df %&gt;% arrange(date, -danceability) %&gt;% # orders the data first by date (asc), then by danceability (desc) slice(1:5) %&gt;% select(Track.Name, date, danceability) ## # A tibble: 5 x 3 ## Track.Name date danceability ## &lt;chr&gt; &lt;date&gt; &lt;dbl&gt; ## 1 Dresscode Gucci 2019-03-30 0.966 ## 2 my strange addiction 2019-03-30 0.939 ## 3 Gib Ihm 2019-03-30 0.928 ## 4 Old Town Road 2019-03-30 0.908 ## 5 bury a friend 2019-03-30 0.905 4.3 Manipulating columns 4.3.1 Extract and rename columns Subset of columns can be extracted via the select function. Selection is possible by name or position. Reversely, one can exclude specific columns via negative selection (using -). Noteworthy are the many helper functions, which are convenient for rapid exploration, but not recommendable for stable software: start_with, last_col, everything, contains, etc. One can rename columns while selecting them. If we want to rename a column while preserving the other columns we use the rename function. df %&gt;% select(Position, Track.Name) # select via column name df %&gt;% select(1, 2) # select via column position df %&gt;% select(-Track.Name) # select all columns except Track.Name df %&gt;% select(starts_with(&quot;dance&quot;)) # select all columns starting with &quot;dance&quot; df %&gt;% select(danceability, everything()) # reorder danceability first, then remaining columns df %&gt;% select(song = Track.Name) # select one column (Track.name) and rename it (song) df %&gt;% rename(song = Track.Name) # renames one column, but preserves all the others 4.3.2 Create new columns The function mutate creates a new variable or overwrites an existing one. Note that we must assign back to make a permanent change to the data. df %&gt;% mutate(duration_s = round(duration_ms / 1000)) %&gt;% # create new variable mutate(Track.Name = as.factor(Track.Name)) %&gt;% # change existing variable select(Track.Name,starts_with(&quot;duration&quot;)) %&gt;% head(5) ## # A tibble: 5 x 3 ## Track.Name duration_ms duration_s ## &lt;fct&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cherry Lady 135597 136 ## 2 Affalterbach 173220 173 ## 3 Blackberry Sky 156497 156 ## 4 Wolke 10 172827 173 ## 5 Puerto Rico 193573 194 4.4 Scoped functions There are scoped variants of ,mutate which affect multiple columns at once: mutate_all: all columns mutate_at: all specified columns mutate_if: all columns that satisfy a condition Equivalent scoped variants exist for selectand summarise as well. df %&gt;% mutate_all(as.character) %&gt;% head(5) # change ALL columns to character type ## # A tibble: 5 x 15 ## Position Track.Name Artist Streams date danceability energy loudness ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Cherry La~ Capit~ 1040382 2019~ 0.838 0.549 -7.145 ## 2 2 Affalterb~ Shindy 822209 2019~ 0.819 0.674 -4.663 ## 3 3 Blackberr~ Eno 704316 2019~ 0.805 0.625 -8.589 ## 4 4 Wolke 10 MERO 681426 2019~ 0.77 0.797 -4.985 ## 5 5 Puerto Ri~ Fero47 557781 2019~ 0.687 0.766 -6.739 ## # ... with 7 more variables: speechiness &lt;chr&gt;, acousticness &lt;chr&gt;, ## # instrumentalness &lt;chr&gt;, liveness &lt;chr&gt;, valence &lt;chr&gt;, tempo &lt;chr&gt;, ## # duration_ms &lt;chr&gt; df %&gt;% mutate_at(vars(danceability, valence), round, digits=1) %&gt;% # Round all specified columns select(Track.Name,danceability, valence, energy) %&gt;% # We see that energy was not rounded head(5) ## # A tibble: 5 x 4 ## Track.Name danceability valence energy ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cherry Lady 0.8 0.7 0.549 ## 2 Affalterbach 0.8 0.8 0.674 ## 3 Blackberry Sky 0.8 0.6 0.625 ## 4 Wolke 10 0.8 0.4 0.797 ## 5 Puerto Rico 0.7 0.6 0.766 If there is no predefined function, one can define an anonymous function (which cannot be used outside this context) on the fly: df %&gt;% mutate_at(vars(danceability, valence), function(x) x*100) %&gt;% # Here we define a custom function in-line that multiplies dancebility and valence by 100 select(Track.Name,danceability, valence) %&gt;% head(5) ## # A tibble: 5 x 3 ## Track.Name danceability valence ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Cherry Lady 83.8 65.4 ## 2 Affalterbach 81.9 76.6 ## 3 Blackberry Sky 80.5 64.7 ## 4 Wolke 10 77 39.3 ## 5 Puerto Rico 68.7 62.9 The typical use case for mutate_if is changing the variable types of all variables satisfying a specific condition. df %&gt;% mutate_if(is.character, as.factor) %&gt;% # IF column has type character, change it to factor glimpse() # We see that Track.Name and Artist were coerced to factor ## Observations: 73,200 ## Variables: 15 ## $ Position &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15,... ## $ Track.Name &lt;fct&gt; Cherry Lady, Affalterbach, Blackberry Sky, Wolke 1... ## $ Artist &lt;fct&gt; Capital Bra, Shindy, Eno, MERO, Fero47, Capital Br... ## $ Streams &lt;dbl&gt; 1040382, 822209, 704316, 681426, 557781, 534339, 4... ## $ date &lt;date&gt; 2019-03-30, 2019-03-30, 2019-03-30, 2019-03-30, 2... ## $ danceability &lt;dbl&gt; 0.838, 0.819, 0.805, 0.770, 0.687, 0.630, 0.592, 0... ## $ energy &lt;dbl&gt; 0.549, 0.674, 0.625, 0.797, 0.766, 0.692, 0.572, 0... ## $ loudness &lt;dbl&gt; -7.145, -4.663, -8.589, -4.985, -6.739, -4.951, -8... ## $ speechiness &lt;dbl&gt; 0.0755, 0.3270, 0.0434, 0.0693, 0.1050, 0.4270, 0.... ## $ acousticness &lt;dbl&gt; 8.77e-01, 1.30e-02, 2.50e-01, 6.62e-02, 3.10e-01, ... ## $ instrumentalness &lt;dbl&gt; 9.64e-04, 0.00e+00, 5.27e-03, 3.81e-06, 0.00e+00, ... ## $ liveness &lt;dbl&gt; 0.1150, 0.3840, 0.0954, 0.0858, 0.1960, 0.1670, 0.... ## $ valence &lt;dbl&gt; 0.6540, 0.7660, 0.6470, 0.3930, 0.6290, 0.8200, 0.... ## $ tempo &lt;dbl&gt; 114.445, 115.897, 102.996, 100.003, 140.039, 179.8... ## $ duration_ms &lt;dbl&gt; 135597, 173220, 156497, 172827, 193573, 203067, 20... Note that the condition above (is.character) refers to the column as a whole, i.e. the condition returns a single TRUE or FALSE. If we want to mutate a column conditional on the single elements within the column, we use the regular mutate function combined with an if_else: df %&gt;% mutate(Top10 = if_else(Position&lt;=10, &quot;Top 10&quot;, &quot;Top 11-200&quot;)) %&gt;% select(Position, Top10, Track.Name) %&gt;% slice(8:12) ## # A tibble: 5 x 3 ## Position Top10 Track.Name ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 8 Top 10 Alleen ## 2 9 Top 10 Ya Salame ## 3 10 Top 10 DEUTSCHLAND ## 4 11 Top 11-200 Gib Ihm ## 5 12 Top 11-200 Jay Jay 4.5 Aggregate The summarise function is the generic way of calculating summary stats for specific variables. Within the function we can apply base R summary functions (sum, mean or max), one of dplyr’s specific summary functions (n, n_distinct) or a user defined summary function. In the standard case the summarise function returns one row. df %&gt;% filter(date == max(date)) %&gt;% summarise(observations = n(), # number of observations (dplyr function) artists = n_distinct(Artist), # number of distinct observations (dplyr) total_streams = sum(Streams), # sum (base R) mean_valence = mean(valence, na.rm=TRUE)) # mean (base R) ## # A tibble: 1 x 4 ## observations artists total_streams mean_valence ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 200 127 20562166 0.520 However, we can also apply summarise to grouped data. Then one row is returned per group. df %&gt;% group_by(month = stringr::str_sub(date, 1, 7)) %&gt;% summarise(artists = n_distinct(Artist), # number of distinct observations (dplyr) total_streams = sum(Streams), # sum (base R) mean_valence = mean(valence, na.rm=TRUE)) # mean (base R) ## # A tibble: 13 x 4 ## month artists total_streams mean_valence ## &lt;chr&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2019-03 125 53691248 0.503 ## 2 2019-04 166 777088420 0.512 ## 3 2019-05 196 797746624 0.512 ## 4 2019-06 171 824731704 0.504 ## 5 2019-07 162 832142863 0.517 ## 6 2019-08 161 763883839 0.518 ## 7 2019-09 153 805779190 0.505 ## 8 2019-10 174 875914502 0.504 ## 9 2019-11 195 805356793 0.512 ## 10 2019-12 296 952177372 0.525 ## 11 2020-01 201 808308854 0.504 ## 12 2020-02 215 775863340 0.511 ## 13 2020-03 182 726353808 0.521 The count function is a useful shortcut for group_by followed by summarise(n = n()). # df %&gt;% group_by(Artist) %&gt;% summarise(n = n()) %&gt;% ungroup() %&gt;% head(5) df %&gt;% count(Artist) %&gt;% head(5) ## # A tibble: 5 x 2 ## Artist n ## &lt;chr&gt; &lt;int&gt; ## 1 *NSYNC 13 ## 2 102 Boyz 9 ## 3 18 Karat 175 ## 4 24kGoldn 26 ## 5 5 Seconds of Summer 109 Sometimes, we want to add the (group) aggregates as a new column to the existing data frame. In this case we just use mutate rather than summarise. df %&gt;% group_by(date) %&gt;% mutate(Total_Streams = sum(Streams), Share = Streams/Total_Streams) %&gt;% select(Streams, Total_Streams, Share, Artist) %&gt;% head(5) ## Adding missing grouping variables: `date` ## # A tibble: 5 x 5 ## # Groups: date [1] ## date Streams Total_Streams Share Artist ## &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 2019-03-30 1040382 30400557 0.0342 Capital Bra ## 2 2019-03-30 822209 30400557 0.0270 Shindy ## 3 2019-03-30 704316 30400557 0.0232 Eno ## 4 2019-03-30 681426 30400557 0.0224 MERO ## 5 2019-03-30 557781 30400557 0.0183 Fero47 4.6 Window functions A window function is a variation on an aggregation function. While mean or sum take n inputs and return a single output, a window function returns n values. Window functions are used inside mutate and filter functions. Offsets:lead and lag Cummulative aggregations: cumsum, cummean,… Rankings: dense_rank, ntile, … 4.7 Combining tables 4.8 Database backend 4.8.1 Motivation As mentioned before, the dplyr syntax reveals strong analogies with SQL. What is more, it is even possible to use dplyr with a database backend. What does this mean? And when is this useful? In a company setting, raw data is usually stored in some form of database. When we want to work with the data in R, the standard way would be to open a connection to the database and read in the data into R’s memory. However, if the size of data is large, there may be problems with this approach: Large data require long reading time Data sets might not even fit into memory Computations might have low performance If we want to work on the raw data (e.g. for statistical / machine learning modelling), this constitutes a problem: either we need a system with larger memory / higher performance. Or we must restrict ourselves to a smaller sample of the data. Or we could connect R to a technology for distributed machine learning, such as Apache Spark. In some cases, however, we don’t actually need to work on the raw data. We would be happy to let the database do the calculations for us (these are built to store and process huge amounts of data), and just read in the resulting data, which is often much smaller in size. This is precisely the use case for dplyr with a database backend. The idea is to write regular dplyr code. The code is translated into SQL under the hood. The data is retrieved from the database and only the results are actually read into R’s memory. 4.8.2 Set up First, we need to install a few things: Database: In a company setting, the database will already be there. If you want to install a database on your computer, popular choices are PostgreSQL or MySQL. Here is an overview of possible choices For this book we will use an in-memory SQLite database. The benefit is that everyone will be able to run the code without the need to set up a proper database. DBI backend package: DBI stands for database interface. We need a package that corresponds to our database. In our case we will use the package RSQLite. With many other databases, the package odbc would be proper choice. dbplyr package This package needs to be installed, but we never need to load it explictly. Once installed, it is sufficient to load the regular dplyr package. Second, we need to connect R to the database. The arguments look slightly different, depending on the database that you are using. Usually, you would also need to specify a user name and password. con &lt;- DBI::dbConnect(RSQLite::SQLite(), dbname = &quot;:memory:&quot;) Third, we need to have data in our database. In a company setting, the data would already be there. In our case, we create a table spotify-charts-germany in the database and copy the corresponding data from R’s memory (df) into this table. copy_to(con, df, &quot;spotify-charts-germany&quot;) 4.8.3 Querying the database First, we register the database table via the tbl function, which creates a table from a data source. spotify_db &lt;- tbl(con, &quot;spotify-charts-germany&quot;) Now we can query this database table using regular dplyr syntax. Note that this works smoothly for the majority but not for all dplyr commands. For instance the slice function is not implemented, i.e. it has no translation to SQL. Hence, in the following statement we extract the first five rows via head(5) instead of slice(1:5). Otherwise the sequence of commands looks identical to the one presented above based on a normal R data frame/tibble. spotify_db %&gt;% # reference to the database table select(Streams, date, Artist, Track.Name) %&gt;% # select columns by name arrange(-Streams) %&gt;% # order rows by some variable head(5) # select rows by position ## # Source: lazy query [?? x 4] ## # Database: sqlite 3.30.1 [:memory:] ## # Ordered by: -Streams ## Streams date Artist Track.Name ## &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1964217 18254 Mariah Carey All I Want for Christmas Is You ## 2 1939974 18254 Wham! Last Christmas ## 3 1788621 18068 Capital Bra Tilidin ## 4 1603796 18254 Chris Rea Driving Home for Christmas - 2019 Remaster ## 5 1538169 18069 Capital Bra Tilidin We can actually see the SQL generated by dplyr in the background via show_query. spotify_db %&gt;% select(Streams, date, Artist, Track.Name) %&gt;% arrange(-Streams) %&gt;% head(5) %&gt;% show_query() # shows the translation into SQL ## &lt;SQL&gt; ## SELECT `Streams`, `date`, `Artist`, `Track.Name` ## FROM `spotify-charts-germany` ## ORDER BY -`Streams` ## LIMIT 5 Alternatively, we could achieve the same by writing the SQL query ourselves, and send the query to the database. You might need to install the packages ‘RMySQL’ before you are able to execute. query &lt;- &quot;SELECT Streams, date, Artist, `Track.Name` FROM `spotify-charts-germany` ORDER BY Streams DESC LIMIT 5&quot; RMySQL::dbSendQuery(con, query) It is important to understand that the data is not in R’s memory until we explicitly collect the data. Once the data is collected, it behaves like any regular R data frame. rdata &lt;- spotify_db %&gt;% select(Streams, date, Artist, Track.Name) %&gt;% arrange(-Streams) %&gt;% head(5) %&gt;% collect() # this pulls the data into R&#39;s memory class(rdata) # this is a regular R data frame / tibble ## [1] &quot;tbl_df&quot; &quot;tbl&quot; &quot;data.frame&quot; "],
["lubridate.html", "5 Dates and times: lubridate 5.1 What is Lubridate? 5.2 Basics 5.3 Import data, clean date-time 5.4 Check and modify data 5.5 Before exploration 5.6 Exploration - Analysis 5.7 Wrap up", " 5 Dates and times: lubridate Resources: Lubridate homepage Cheatsheet Book Chapter in R4DS Vignette Suggested data set: weather-kiel-holtenau 5.1 What is Lubridate? Lubridate is an R-Package designed to ease working with date/time variables. These can be challenging in baseR and lubridate allows for frictionless working with dates and times, hence the name. Lubridate ist part of the tidyverse package, but can be installed seperately as well. It probably reveals most of its usefullness in collaboration with other tidyverse packages. A useful extension, depending on your data, might be the time-series package which is not part of tidyverse. All mentioned packages can be optained with the following commands in the RStudio console. install.packages(“lubridate”) install.packages(“tidyverse”) install.packages(“testit”) If they have been installed previously in your environment, they might have to be called upon by using library(tidyverse) and so forth - see code chunks below. 5.2 Basics Some examples of real world date-time formats found in datasets: How people talk about dates and times often differs from the notation. Depending on the specific use of the data, the given information might be more or less granular. When people in the USA talk distance between two places, they often give an approximation of how long it will take a person to drive from A to B and round-up or down to the hour. Flight schedules will most likely be exact to the minute, while some sensordata will probably need to be exact to the second. So there will be differing granularity in date and time data. Even if this would not be challenging, we still would have to deal with different notations of date and time. People in Germany will write a day-date like: dd.mm.yyyy or short dd.mm.yy, while the anglo-saxon realm will use mm.dd.yyyy frequently and the most chronologically sound way would be to use yyyy.mm.dd, but this doesn’t seem to stick with humans. On top of these issues there’s the fact that time itself does not make the impression of being the most exact parameter out there. Physical time might appear linear, but the way our planet revolves within our galaxy has made it neccessary to adjust date and times every now and then, so our calendar stays in tune with defined periods and seasons. This creates leap years, skipped seconds, daylight-savings time and last, but not least time-zones, which can mess things up even further. After defining date-time data as distinct “places in time” often the need for calculation with them arises. 5.3 Import data, clean date-time We will apply the lubridate package to Weather data from on stationary sensor in northern Germany, the weather station in Kiel-Holtenau to be more exact. Before we introduce the library, the technical prerequisites must be created. library(readr) # part of the tidyverse library(lubridate) # the mentioned lubrication for date-time wrangling ## ## Attache Paket: &#39;lubridate&#39; ## The following object is masked from &#39;package:base&#39;: ## ## date library(tidyverse) # the tidyverse with its dplyr functions for data wrangling ## -- Attaching packages ----------------------- tidyverse 1.3.0 -- ## v ggplot2 3.3.0 v dplyr 0.8.5 ## v tibble 2.1.3 v stringr 1.4.0 ## v tidyr 1.0.2 v forcats 0.5.0 ## v purrr 0.3.3 ## -- Conflicts -------------------------- tidyverse_conflicts() -- ## x lubridate::as.difftime() masks base::as.difftime() ## x lubridate::date() masks base::date() ## x dplyr::filter() masks stats::filter() ## x lubridate::intersect() masks base::intersect() ## x dplyr::lag() masks stats::lag() ## x lubridate::setdiff() masks base::setdiff() ## x lubridate::union() masks base::union() library(ggplot2) # data visualisation package (is actually part of tidyverse, but still) library(testit) # testing data selections With the lubridate package activated, we can now call some simple functions to check where we are in time: # Lubridate today() # date like YYYY-mm-dd ## [1] &quot;2020-05-14&quot; now() # timestamp like YYYY-mm-dd HH:MM:SS TZ ## [1] &quot;2020-05-14 21:47:35 CEST&quot; # Base R Sys.Date() # date like YYYY-mm-dd ## [1] &quot;2020-05-14&quot; Sys.time() # timestamp like YYYY-mm-dd HH:MM:SS TZ ## [1] &quot;2020-05-14 21:47:35 CEST&quot; Sys.timezone() # actual timezone &quot;Europe/Berlin&quot; ## [1] &quot;Europe/Berlin&quot; The first function delivers a date, while the second delivers a date-time (a date + a time). The third is a baseR function that produces the date, the operating system sees as current, while the fourth displays the systems full current timestamp. The third and fourth functions are both baseR and therefore case sensitive, while the first and second are lubridate functions which behave just nicer and allow for fluent calculations. Now let’s have a look at some real world data and the challenges of date-time formatting. A first step is to import the data, which is given in a csv-format. We will use the tidyverse version of read_csv to accomplish this step. The data will be called df (data.frame) in order to make reference in code an writing more efficient further down. This is the simple approach to read a file with suffix csv. df &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;) ## Parsed with column specification: ## cols( ## STATIONS_ID = col_double(), ## MESS_DATUM = col_double(), ## TEMPERATUR = col_double(), ## RELATIVE_FEUCHTE = col_double(), ## NIEDERSCHLAGSDAUER = col_double(), ## NIEDERSCHLAGSHOEHE = col_double(), ## NIEDERSCHLAGSINDIKATOR = col_double() ## ) head(df,5) ## # A tibble: 5 x 7 ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2.02e11 3.6 75.6 0 ## 2 2564 2.02e11 3.6 75.6 6 ## 3 2564 2.02e11 3.6 76.5 0 ## 4 2564 2.02e11 3.5 75.6 0 ## 5 2564 2.02e11 3.6 75.3 0 ## # ... with 2 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt; All columns are recognized as double values. By looking at the “MESS_DATUM” column it becomes apparent that this represents a timestamp that was created every 10 minutes. The standard column type definition of the import tool has not sufficed to format this appropriately, which is why the column will be defined as a double for now. Using the code generator of the data import tool it is possible to format the variables in the appropriate classes. The code snippet is automatically generated by the import tool of RStudio. For our concern of the timestamp variable please note the part where the column MESS_DATUM = col_datetime(format = “%Y%m%d%H%M”) which will reach our present goal to format the timestamp and class as a POSIXct. df &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;, col_types = cols(MESS_DATUM = col_datetime(format = &quot;%Y%m%d%H%M&quot;), NIEDERSCHLAGSDAUER = col_integer(), NIEDERSCHLAGSINDIKATOR = col_integer(), STATIONS_ID = col_integer())) head(df,5) ## # A tibble: 5 x 7 ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 2019-04-14 00:00:00 3.6 75.6 0 ## 2 2564 2019-04-14 00:10:00 3.6 75.6 6 ## 3 2564 2019-04-14 00:20:00 3.6 76.5 0 ## 4 2564 2019-04-14 00:30:00 3.5 75.6 0 ## 5 2564 2019-04-14 00:40:00 3.6 75.3 0 ## # ... with 2 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;int&gt; We will examine the data further down below. At this point it should be mentioned again that we generated the code to import the data by using the import readr tool of RStudio. This allows a first look at the raw csv data before import and some tweaking of the variables and their classes. We left the locale portion of the import tool untouched. The following lines of code import the same data in the MESS_DATUM column as a string of characters again and then call a specific lubridate function (ymd = year-month-day) which basically recognizes the time format in the column given only very little information. In our case we only specify the order in which the date-time information is given, which is Year-Month-Day-Hour-Minute. The ymd_hm function delivers the desired result however, only if it is applied to a character string, which is why we chose to overwrite the dataset df with a new import procedure that makes sure that the column MESS_DATUM is given as a character after import in order to be then transformed to a date-time. df1 &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;, col_types = cols( MESS_DATUM = col_character(), NIEDERSCHLAGSINDIKATOR = col_integer(), STATIONS_ID = col_integer())) df1$MESS_DATUM &lt;- ymd_hm(df1$MESS_DATUM) head(df1,5) ## # A tibble: 5 x 7 ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2019-04-14 00:00:00 3.6 75.6 0 ## 2 2564 2019-04-14 00:10:00 3.6 75.6 6 ## 3 2564 2019-04-14 00:20:00 3.6 76.5 0 ## 4 2564 2019-04-14 00:30:00 3.5 75.6 0 ## 5 2564 2019-04-14 00:40:00 3.6 75.3 0 ## # ... with 2 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;int&gt; As can be seen above, the MESS_DATUM is now in a POSIXct format again, which is what will be needed for further calculations and analysis of the data set. Alternatively you could use the following parsing function to achieve the same result. Note that the parse_date_time function of lubridate also needs the timestamp to be in character format in order to work. df2 &lt;- read_csv(&quot;data/weather_kiel_holtenau.csv&quot;, col_types = cols(MESS_DATUM = col_character())) df2$MESS_DATUM &lt;- parse_date_time(df2$MESS_DATUM, orders =&quot;Ymd HM&quot;) head(df2,5) ## # A tibble: 5 x 7 ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;dbl&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2019-04-14 00:00:00 3.6 75.6 0 ## 2 2564 2019-04-14 00:10:00 3.6 75.6 6 ## 3 2564 2019-04-14 00:20:00 3.6 76.5 0 ## 4 2564 2019-04-14 00:30:00 3.5 75.6 0 ## 5 2564 2019-04-14 00:40:00 3.6 75.3 0 ## # ... with 2 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt; OK, we have successfully formated the time-stamp data into a productive date-time (dttm) format using three different aproaches. 5.4 Check and modify data The next steps are a check and elimination procedure to eliminate missing values (NAs) from the dataset, if some observations might have failed to generate a timestamp. df %&gt;% filter(is.na(MESS_DATUM)) %&gt;% # Checking if there are observations with a missing MESS_DATUM head() ## # A tibble: 0 x 7 ## # ... with 7 variables: STATIONS_ID &lt;int&gt;, MESS_DATUM &lt;dttm&gt;, TEMPERATUR &lt;dbl&gt;, ## # RELATIVE_FEUCHTE &lt;dbl&gt;, NIEDERSCHLAGSDAUER &lt;int&gt;, NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;int&gt; We don’t encounter any NAs in the MESS_DATUM column. Eliminating NAs from the MESS_DATUM Column including the other variables of that specific observations from the data frame would have been possible with: df &lt;- df %&gt;% filter(!is.na(MESS_DATUM)) # missing MESS_DATUM observations (NAs) would/will be excluded from the data frame Now it is time to have a first explorative look at the data set. max(df$MESS_DATUM) # The latest observation is dated 10 minutes before midnight on the 13th of April 2020 ## [1] &quot;2020-04-13 23:50:00 UTC&quot; min(df$MESS_DATUM) # The earliest observation is dated 14th of April 2019 ## [1] &quot;2019-04-14 UTC&quot; range(df$MESS_DATUM) # Another way to get the same information ## [1] &quot;2019-04-14 00:00:00 UTC&quot; &quot;2020-04-13 23:50:00 UTC&quot; We are dealing with data over the course of one year starting on midnight 14th April 2019 until 10 minutes to midnight on 13th April 2020. To gather some information on the other variables we can use the range function again. range(df$STATIONS_ID) # This confirms: We are only dealing with data from one and only one weather sensor in the whole data set. ## [1] 2564 2564 range(df$TEMPERATUR) # We have a range of temperatures between -2.5 and +32.1 degrees celius, which sounds about plausible. ## [1] -2.5 32.1 range(df$RELATIVE_FEUCHTE) # Delivers an unexpected range, where the minimum is -999 and the maximum is +100. Both values are either not possible (-999) or only of theoretic value (+100) ## [1] -999 100 range(df$NIEDERSCHLAGSDAUER) # This delivers an implausible minimum of -999 and an expected maximum of 10 (the maximum amount of minutes in a 10 minute interval). ## [1] -999 10 range(df$NIEDERSCHLAGSHOEHE) # Again the range shows a minimum at -999 and a maximum of 7.85 which has to be interpreted as mm which equals litres per square meter (6 x 7.85 = 47.1/h = torrential) ## [1] -999.00 7.85 range(df$NIEDERSCHLAGSINDIKATOR) # A look at the data reveals a binary 1 or a 0 for this variable, i.e. it rained or it didn&#39;t. -999 must be interpreted as a failed observation ## [1] -999 1 Here we find observations in a number of variables that would need to be deleted from the table to get operable data. Theoretically, the following columns are only relevant if the NIEDERSCHLAGSINDIKATOR is set to 1 = rainfall. This column can be considered like a switch for the other two rain-connected columns: - NIEDERSCHLAGSDAUER - NIEDERSCHLAGSHOEHE Getting an idea about the extent of weird/failed observations can be accomplished with the following code snippet: values_of_ind &lt;- df %&gt;% group_by(NIEDERSCHLAGSINDIKATOR) %&gt;% tally() head(values_of_ind) ## # A tibble: 3 x 2 ## NIEDERSCHLAGSINDIKATOR n ## &lt;int&gt; &lt;int&gt; ## 1 -999 127 ## 2 0 43290 ## 3 1 9287 So 127 observations of rainfall are faulty. To modify the NIEDERSCHLAGSINDIKATOR for upcoming analysis we do the following in order to set the -999 Values to a more neutral Zero: df$NIEDERSCHLAGSINDIKATOR &lt;- ifelse(df$NIEDERSCHLAGSINDIKATOR == -999, 0, df$NIEDERSCHLAGSINDIKATOR) value_of_ind_n &lt;- df %&gt;% group_by(NIEDERSCHLAGSINDIKATOR) %&gt;% tally() head(value_of_ind_n) ## # A tibble: 2 x 2 ## NIEDERSCHLAGSINDIKATOR n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 43417 ## 2 1 9287 We now have a binary indicator of rainfall. Over the course of the observed year it rained 9287/43417*100 = 21.4% of the time frames observed. The column RELATIVE_FEUCHTE must also be adjusted in the same way. values_of_rel &lt;- df %&gt;% group_by(RELATIVE_FEUCHTE) %&gt;% filter(RELATIVE_FEUCHTE &lt; 0) %&gt;% tally() head(values_of_rel) ## # A tibble: 1 x 2 ## RELATIVE_FEUCHTE n ## &lt;dbl&gt; &lt;int&gt; ## 1 -999 128 The negative values are set to 0 again. df$RELATIVE_FEUCHTE &lt;- ifelse(df$RELATIVE_FEUCHTE == -999, 0, df$RELATIVE_FEUCHTE) values_of_rel_n &lt;- df %&gt;% group_by(RELATIVE_FEUCHTE) %&gt;% tally() head(values_of_rel_n) ## # A tibble: 6 x 2 ## RELATIVE_FEUCHTE n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 128 ## 2 25 1 ## 3 25.4 1 ## 4 25.5 1 ## 5 25.8 1 ## 6 26.3 1 The column NIEDERSCHLAGSHOEHE can also be tested and adjusted in the same way. This is not strictly necessary since the negative values could be hidden from further analysis using the NIEDERSCHLAGSINDIKATOR = 1. values_of_ARN &lt;- df %&gt;% group_by(NIEDERSCHLAGSHOEHE) %&gt;% filter(NIEDERSCHLAGSHOEHE &lt; 0) %&gt;% tally() head(values_of_ARN) ## # A tibble: 1 x 2 ## NIEDERSCHLAGSHOEHE n ## &lt;dbl&gt; &lt;int&gt; ## 1 -999 129 The negative values are set to 0 again. df$NIEDERSCHLAGSHOEHE &lt;- ifelse(df$NIEDERSCHLAGSHOEHE == -999, 0, df$NIEDERSCHLAGSHOEHE) values_of_RFH &lt;- df %&gt;% group_by(NIEDERSCHLAGSHOEHE) %&gt;% tally() head(values_of_RFH) ## # A tibble: 6 x 2 ## NIEDERSCHLAGSHOEHE n ## &lt;dbl&gt; &lt;int&gt; ## 1 0 47753 ## 2 0.01 712 ## 3 0.02 343 ## 4 0.03 377 ## 5 0.04 397 ## 6 0.05 234 We have set Variable NIEDERSCHLAGSHOEHE to 0 in these 129 oberservations. In a total of 47,753 observations with 0 as a value the distortion this might cause can be called negligible. 5.5 Before exploration We now have clean and tidy data and can begin with general exploration, analysis and interpretation. The “TEMPERATUR” is given in degrees Celsius, the “RELATIVE-FEUCHTE” is a percentage Value for humidity which refers to the degree of water saturation that is prevalent in the air at a given temperature. As temperature increases, the air can absorb larger amounts of water, hence the relativity of this variable. The next variable is “NIEDERSCHLAGSDAUER” which is given as an integer smaller or equal to 10. Therefore it gives the time it has rained during the timestamp interval of 10 Minutes. The variable “NIEDERSCHLAGSHOEHE” is a measure of rainfall intensity. Its maximum value can also be checked by the following command: max(df$NIEDERSCHLAGSHOEHE, na.rm = FALSE) ## [1] 7.85 We can assume that this number gives us the amount of rainfall in milimeters, which is a common definiton and is equivalent as liters of rainfall per squaremeter in a given time interval. A strong rainfall in central Europe can be expected to generate around 30mm/h of rainfall, i.e. 30 liters per squaremeter per hour. The maximum value is therefore an indicator of a very heavy downpour as a continuation for a whole hour would have yielded 6 x 7.85 = 47.1 litres per hour. The next variable is simply a binary expression of rainfall (1) or no rainfall (0) in the given interval. This is relevant to measure as some types of rainfall seem to not generate enough water to messure an actual amount of water. The dreaded Northgerman “drizzle” comes to mind. Again: Since the sensor has taken a snapshot every 10 minutes, we have six observations per hour. 5.5.1 Intervals Lubridate allows for the definition of an interval object with the “interval” class. The interval is simply defined as the time elapsed between two points on the date-time line. e.g. The interval of a single day - from the first of March to the second of March can be defined as follows: tag_int &lt;- interval(ymd(&quot;2020-03-01&quot;), ymd(&quot;2020-03-02&quot;)) class(tag_int) ## [1] &quot;Interval&quot; ## attr(,&quot;package&quot;) ## [1] &quot;lubridate&quot; range(int_start(tag_int),int_end(tag_int)) ## [1] &quot;2020-03-01 UTC&quot; &quot;2020-03-02 UTC&quot; This small interval of one day turned out to be good for further analysis. This makes it easier to check results at a glance without having to query the whole dataset. When the desired results can be achieved for a small interval we could consider the next largest period, e.g. Week, month, quarter and year. 5.5.1.1 Durations and periods For any weather data, an analysis of seasonal differences is a natural (excuse the pun) objective. The beginning of each years seasons align with the solar incidences, i.e. the spring and autumnal equinoxes and the days of most and least sunlight hours in summer and winter, respectively. The following code snippets define the starting points of the different seasons as points on the POSIXct timeline and use these points to calculate new points by later adding durations and periods to them. For a seasons view, we create the appropriate points in time, where each season begins. season_19_spring &lt;- ymd_hm(&quot;2019-03-20 22:58&quot;, tz=&quot;CET&quot;) season_19_summer &lt;- ymd_hm(&quot;2019-06-21 17:54&quot;, tz=&quot;CET&quot;) season_19_autumn &lt;- ymd_hm(&quot;2019-09-23 09:50&quot;, tz=&quot;CET&quot;) season_19_winter &lt;- ymd_hm(&quot;2019-12-22 05:19&quot;, tz=&quot;CET&quot;) season_20_spring &lt;- ymd_hm(&quot;2020-03-20 04:49&quot;, tz=&quot;CET&quot;) season_20_summer &lt;- ymd_hm(&quot;2020-06-20 23:43&quot;, tz=&quot;CET&quot;) Source re: starts of seasons from timeanddate.de Our goal shall be to create an interval for the spring season. The following information is available to determine the end point of spring: Frühling 2020 Beginn (Tag-und-Nachtgleiche März) 20. März 04:49 Dauer 92 Tage, 17 Std, 54 Min Sommer 2020 Beginn (Sonnenwende Juni) 20. Juni 23:43 Dauer 93 Tage, 15 Std, 46 Min 5.5.1.1.1 Durations First approach is with the object DURATIONS of lubridate: print(paste(&quot;spring 2020 - start: &quot;,season_20_spring)) ## [1] &quot;spring 2020 - start: 2020-03-20 04:49:00&quot; print(ddays(92)) # create a DURATION object using lubridate function ddays() ## [1] &quot;7948800s (~13.14 weeks)&quot; class(ddays(92)) ## [1] &quot;Duration&quot; ## attr(,&quot;package&quot;) ## [1] &quot;lubridate&quot; typeof(ddays(92)) ## [1] &quot;double&quot; season_20_spring_dur &lt;- season_20_spring + ddays(92) + dhours(17) + dminutes(54) print(season_20_spring_dur) ## [1] &quot;2020-06-20 23:43:00 CEST&quot; class(season_20_spring_dur) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; typeof(season_20_spring_dur) ## [1] &quot;double&quot; print(paste(&quot;spring 2020-end of DURATIONS:&quot;,season_20_spring_dur, &quot;expected:&quot;, season_20_summer, &quot;is failure:&quot;, season_20_spring_dur!=season_20_summer)) ## [1] &quot;spring 2020-end of DURATIONS: 2020-06-20 23:43:00 expected: 2020-06-20 23:43:00 is failure: FALSE&quot; 5.5.1.1.2 Periods Second approach is with the object PERIODS of lubridate: print(days(92)) # create a PERIOD object using lubridate function days() ## [1] &quot;92d 0H 0M 0S&quot; class(days(92)) ## [1] &quot;Period&quot; ## attr(,&quot;package&quot;) ## [1] &quot;lubridate&quot; typeof(days(92)) ## [1] &quot;double&quot; season_20_spring_p &lt;- season_20_spring + days(92) + hours(17) + minutes(54) print(season_20_spring_p) ## [1] &quot;2020-06-20 22:43:00 CEST&quot; class(season_20_spring_p) ## [1] &quot;POSIXct&quot; &quot;POSIXt&quot; typeof(season_20_spring_p) ## [1] &quot;double&quot; print(paste(&quot;spring 2020-end of PERIODS:&quot;,season_20_spring_p, &quot;expected:&quot;, season_20_summer , &quot;is failure:&quot;, season_20_spring_p!=season_20_summer)) ## [1] &quot;spring 2020-end of PERIODS: 2020-06-20 22:43:00 expected: 2020-06-20 23:43:00 is failure: TRUE&quot; This example shows that the use of PERIODS or DURATIONS must be weighed up and balanced depending on what are you looking for. The periods-example has a difference of one hour to the durations-example, because its clock-time takes daylight savings time shift. 5.5.1.2 Math with periods Using starting points of seasons summer, autumn and winter to create intervals for our data frame. int_season_summer &lt;- interval(season_19_summer, season_19_autumn - minutes(1)) int_season_autumn &lt;- interval(season_19_autumn, season_19_winter - minutes(1)) int_season_winter &lt;- season_19_winter %--% (season_20_spring - minutes(1)) 5.5.2 Groupings What follows is preliminary step working with group_bys and renames in order to then apply the findings to seasons again. The following GROUP_BY command creates a unique grouping per hour and then calculates the average temperature for every hour, then pastes this value into each observation per hour. df_sel &lt;- df %&gt;% filter(MESS_DATUM &gt;= int_start(tag_int) &amp; MESS_DATUM &lt;= int_end(tag_int)) df_group &lt;- df_sel %&gt;% group_by(STATIONS_ID , hour(MESS_DATUM) , day(MESS_DATUM) ) head(df_group, 10) ## # A tibble: 10 x 9 ## # Groups: STATIONS_ID, hour(MESS_DATUM), day(MESS_DATUM) [2] ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 2020-03-01 00:00:00 5.2 71.1 0 ## 2 2564 2020-03-01 00:10:00 5.4 69.5 0 ## 3 2564 2020-03-01 00:20:00 5.5 68.2 0 ## 4 2564 2020-03-01 00:30:00 5.6 68.3 0 ## 5 2564 2020-03-01 00:40:00 5.5 69 0 ## 6 2564 2020-03-01 00:50:00 5.5 69.1 0 ## 7 2564 2020-03-01 01:00:00 5.4 69.7 0 ## 8 2564 2020-03-01 01:10:00 5.3 70 0 ## 9 2564 2020-03-01 01:20:00 5.3 69.9 0 ## 10 2564 2020-03-01 01:30:00 5.2 71 0 ## # ... with 4 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt;, `hour(MESS_DATUM)` &lt;int&gt;, ## # `day(MESS_DATUM)` &lt;int&gt; For better readability, the columns can be renamed after a Group-By with rename function. df_group &lt;- df_sel %&gt;% group_by(STATIONS_ID , hour(MESS_DATUM) , day(MESS_DATUM) ) %&gt;% rename( &quot;STUNDE&quot; = &#39;hour(MESS_DATUM)&#39;, &quot;TAG&quot; = &#39;day(MESS_DATUM)&#39; ) head(df_group, 10) ## # A tibble: 10 x 9 ## # Groups: STATIONS_ID, STUNDE, TAG [2] ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 2020-03-01 00:00:00 5.2 71.1 0 ## 2 2564 2020-03-01 00:10:00 5.4 69.5 0 ## 3 2564 2020-03-01 00:20:00 5.5 68.2 0 ## 4 2564 2020-03-01 00:30:00 5.6 68.3 0 ## 5 2564 2020-03-01 00:40:00 5.5 69 0 ## 6 2564 2020-03-01 00:50:00 5.5 69.1 0 ## 7 2564 2020-03-01 01:00:00 5.4 69.7 0 ## 8 2564 2020-03-01 01:10:00 5.3 70 0 ## 9 2564 2020-03-01 01:20:00 5.3 69.9 0 ## 10 2564 2020-03-01 01:30:00 5.2 71 0 ## # ... with 4 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt;, STUNDE &lt;int&gt;, TAG &lt;int&gt; Or the columns can be named directly in the Group-By command: df_group &lt;- df_sel %&gt;% group_by(STATIONS_ID , STUNDE = hour(MESS_DATUM) , TAG = day(MESS_DATUM) ) head(df_group, 5) ## # A tibble: 5 x 9 ## # Groups: STATIONS_ID, STUNDE, TAG [1] ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 2020-03-01 00:00:00 5.2 71.1 0 ## 2 2564 2020-03-01 00:10:00 5.4 69.5 0 ## 3 2564 2020-03-01 00:20:00 5.5 68.2 0 ## 4 2564 2020-03-01 00:30:00 5.6 68.3 0 ## 5 2564 2020-03-01 00:40:00 5.5 69 0 ## # ... with 4 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt;, STUNDE &lt;int&gt;, TAG &lt;int&gt; Now we can filter the data by year, month, week, day and hour of the day. This should give us possibilities to aggregate freely over any time specifications. 5.5.2.1 Seasons Now we create data frames for analysing seasons. 5.5.2.1.1 Spring We do not consider spring as our data frame is not complete for this period. 5.5.2.1.2 Summer The challenge when selecting summer dates is the time zone. df_summer_attempt_1 &lt;- df %&gt;% filter(MESS_DATUM &gt;= int_start(int_season_summer) &amp; MESS_DATUM &lt;= int_end(int_season_summer)) range(df_summer_attempt_1$MESS_DATUM) ## [1] &quot;2019-06-21 16:00:00 UTC&quot; &quot;2019-09-23 07:40:00 UTC&quot; If the expected number of data records does not match the currently determined number, the further analysis should not be continued or repeated. With the help of assurances(assert) logical errors in programs or analysis can be identified and ended in a controlled manner, if necessary. rows_sum &lt;- count(df_summer_attempt_1) # or count(df_select_summer, n()) expected_rows_sum &lt;- 13487 # determined with e.g. excel or SQL assert(&quot;expected rows sum&quot; , expected_rows_sum == rows_sum) expected_summer_start &lt;- ymd_hm(&quot;201906211800&quot;, tz=&quot;UTC&quot;) expected_summer_end &lt;- ymd_hm(&quot;201909230940&quot;, tz=&quot;UTC&quot;) # TODO TZ Umstellung!!! print(paste(&quot;summer min and max&quot;, min(df_summer_attempt_1$MESS_DATUM), max(df_summer_attempt_1$MESS_DATUM))) ## [1] &quot;summer min and max 2019-06-21 16:00:00 2019-09-23 07:40:00&quot; #assert(&quot;expected summer-start&quot; , expected_summer_start == min(df_summer_attempt_1$MESS_DATUM)) #assert(&quot;expected summer-end&quot; , expected_summer_end == max(df_summer_attempt_1$MESS_DATUM)) Our first approach gets the correct number of records, but not the expected record with the first interval value ‘2019-06-21 18:00’. To find For the analysis of start and end time points we use the stamp function of lubridate. sf &lt;- stamp(&quot;set to 24 Jun 2019 3:34&quot;) ## Multiple formats matched: &quot;set to %d %Om %Y %H:%M&quot;(1), &quot;set to %d %b %Y %H:%M&quot;(1) ## Using: &quot;set to %d %b %Y %H:%M&quot; print(sf(int_start(int_season_summer))) ## [1] &quot;set to 21 Jun 2019 17:54&quot; print(sf(int_end(int_season_summer))) ## [1] &quot;set to 23 Sep 2019 09:49&quot; sf &lt;- stamp(&quot;set to Monday, 24.06.2019 3:34&quot;) ## Multiple formats matched: &quot;set to Monday, %d.%Om.%Y %H:%M&quot;(1), &quot;set to Monday, %d.%m.%Y %H:%M&quot;(1) ## Using: &quot;set to Monday, %d.%Om.%Y %H:%M&quot; print(sf(int_start(int_season_summer))) ## [1] &quot;set to Monday, 21.06.2019 17:54&quot; print(sf(int_end(int_season_summer))) ## [1] &quot;set to Monday, 23.09.2019 09:49&quot; sf &lt;- stamp(&quot;Created Sunday, Jan 17, 1999 3:34&quot;) ## Multiple formats matched: &quot;Created Sunday, %Om %d, %Y %H:%M&quot;(1), &quot;Created Sunday, %b %d, %Y %H:%M&quot;(1) ## Using: &quot;Created Sunday, %b %d, %Y %H:%M&quot; print(sf(ymd(&quot;2010-04-05&quot;))) ## [1] &quot;Created Sunday, Apr 05, 2010 00:00&quot; print(sf(int_start(int_season_summer))) ## [1] &quot;Created Sunday, Jun 21, 2019 17:54&quot; print(sf(int_end(int_season_summer))) ## [1] &quot;Created Sunday, Sep 23, 2019 09:49&quot; sf &lt;- stamp(&quot;Created 01.01.1999 03:34&quot;) ## Multiple formats matched: &quot;Created %Om.%d.%Y %H:%M&quot;(1), &quot;Created %d.%Om.%Y %H:%M&quot;(1), &quot;Created %m.%d.%Y %H:%M&quot;(1), &quot;Created %d.%m.%Y %H:%M&quot;(1) ## Using: &quot;Created %Om.%d.%Y %H:%M&quot; print(sf(ymd(&quot;2010-04-05&quot;))) ## [1] &quot;Created 04.05.2010 00:00&quot; print(sf(int_start(int_season_summer))) ## [1] &quot;Created 06.21.2019 17:54&quot; print(sf(int_end(int_season_summer))) ## [1] &quot;Created 09.23.2019 09:49&quot; The function stamp could be a useful function of lubridate, in our opinion a little bit hard to configure. sft &lt;- &quot;%d.%m.%Y-%H:%M&quot; print(format(int_start(int_season_summer), sft)) ## [1] &quot;21.06.2019-17:54&quot; print(format(int_end(int_season_summer), sft)) ## [1] &quot;23.09.2019-09:49&quot; print(paste(&quot;Summer start/End format&quot; , format(int_start(int_season_summer), sft) , format(int_end(int_season_summer), sft) )) ## [1] &quot;Summer start/End format 21.06.2019-17:54 23.09.2019-09:49&quot; print(paste(&quot;Summer start/End&quot; , int_start(int_season_summer) , int_end(int_season_summer) )) ## [1] &quot;Summer start/End 2019-06-21 17:54:00 2019-09-23 09:49:00&quot; If we do not set the time zone correctly, we get the expected number of data records with our current data frame, but with a time delay. Select data with timezone format of the data frames first row. range(df$MESS_DATUM) # check the whole data frame ## [1] &quot;2019-04-14 00:00:00 UTC&quot; &quot;2020-04-13 23:50:00 UTC&quot; df_tz &lt;- tz(df$MESS_DATUM[[1]]) # timezone of the first element # print(paste(&quot;TimeZone:&quot;, df_tz)) df_summer_attempt_2 &lt;- df %&gt;% filter(MESS_DATUM &gt;= force_tz(int_start(int_season_summer)) # use the lubridate function force_tz &amp; MESS_DATUM &lt;= force_tz(int_end(int_season_summer))) range(df_summer_attempt_2$MESS_DATUM) ## [1] &quot;2019-06-21 16:00:00 UTC&quot; &quot;2019-09-23 07:40:00 UTC&quot; df_summer_attempt_3 &lt;- df %&gt;% filter(MESS_DATUM &gt;= with_tz(int_start(int_season_summer),df_tz) # use the lubridate function with_tz &amp; MESS_DATUM &lt;= with_tz(int_end(int_season_summer),df_tz)) range(df_summer_attempt_3$MESS_DATUM) ## [1] &quot;2019-06-21 16:00:00 UTC&quot; &quot;2019-09-23 07:40:00 UTC&quot; n_start_w &lt;- with_tz(int_start(int_season_summer),df_tz) n_end_w &lt;- with_tz(int_end(int_season_summer),df_tz) print(paste(&quot;with TZ start/end&quot;, n_start_w, &quot;/&quot;, n_end_w)) ## [1] &quot;with TZ start/end 2019-06-21 15:54:00 / 2019-09-23 07:49:00&quot; n_start_f &lt;- force_tz(int_start(int_season_summer),df_tz) # the optimal way n_end_f &lt;- force_tz(int_end(int_season_summer),df_tz) print(paste(&quot;Force TZ start/end&quot;, n_start_f, &quot;/&quot;, n_end_f)) ## [1] &quot;Force TZ start/end 2019-06-21 17:54:00 / 2019-09-23 09:49:00&quot; df_select_summer &lt;- df %&gt;% filter(MESS_DATUM &gt;= n_start_f &amp; MESS_DATUM &lt;= n_end_f) range(df_select_summer$MESS_DATUM) ## [1] &quot;2019-06-21 18:00:00 UTC&quot; &quot;2019-09-23 09:40:00 UTC&quot; We check our data frame again. rows_sum &lt;- count(df_summer_attempt_1) # or count(df_select_summer, n()) expected_rows_sum &lt;- 13487 # determined with e.g. excel or SQL assert(&quot;expected rows sum&quot; , expected_rows_sum == rows_sum) expected_summer_start &lt;- ymd_hm(&quot;201906211800&quot;, tz=&quot;UTC&quot;) expected_summer_end &lt;- ymd_hm(&quot;201909230940&quot;, tz=&quot;UTC&quot;) # TODO TZ Umstellung!!! print(paste(&quot;summer min and max&quot;, min(df_select_summer$MESS_DATUM), max(df_select_summer$MESS_DATUM))) ## [1] &quot;summer min and max 2019-06-21 18:00:00 2019-09-23 09:40:00&quot; assert(&quot;expected summer-start&quot; , expected_summer_start == min(df_select_summer$MESS_DATUM)) assert(&quot;expected summer-end&quot; , expected_summer_end == max(df_select_summer$MESS_DATUM)) 5.5.2.1.3 Autumn New attempt to set the start and end points of an interval with the lubridate functions floor_date() and round_date(). print(paste(&quot;autumn start/end&quot;, season_19_autumn, &quot;/&quot;, season_19_winter)) ## [1] &quot;autumn start/end 2019-09-23 09:50:00 / 2019-12-22 05:19:00&quot; show_round_up_start &lt;- round_date(season_19_autumn, unit = &quot;hour&quot;) print(paste(&quot;Round hour&quot;,show_round_up_start)) ## [1] &quot;Round hour 2019-09-23 10:00:00&quot; #TODO - Warum nicht auf 5:10 show_round_down_end &lt;- floor_date(season_19_winter, unit = &quot;hour&quot;) print(paste(&quot;Round minute&quot;,show_round_down_end)) ## [1] &quot;Round minute 2019-12-22 05:00:00&quot; print(paste(&quot;autumn start/end&quot;, show_round_up_start, &quot;/&quot;, show_round_down_end)) ## [1] &quot;autumn start/end 2019-09-23 10:00:00 / 2019-12-22 05:00:00&quot; These results are not completly satisfying. So we use our aproach from summer selection of data. a_start_f &lt;- force_tz(int_start(int_season_autumn),df_tz) a_end_f &lt;- force_tz(int_end(int_season_autumn),df_tz) print(paste(&quot;Force TZ start/end&quot;, a_start_f, &quot;/&quot;, a_end_f)) ## [1] &quot;Force TZ start/end 2019-09-23 09:50:00 / 2019-12-22 05:18:00&quot; df_select_autumn &lt;- df %&gt;% filter(MESS_DATUM &gt;= a_start_f &amp; MESS_DATUM &lt;= a_end_f) print(count(df_select_autumn)) ## # A tibble: 1 x 1 ## n ## &lt;int&gt; ## 1 12933 df_group_autumn &lt;- df_select_autumn %&gt;% group_by(STATIONS_ID , TAG=day(MESS_DATUM) , MONAT=month(MESS_DATUM) , JAHR=year(MESS_DATUM)) head(df_group_autumn, 10) ## # A tibble: 10 x 10 ## # Groups: STATIONS_ID, TAG, MONAT, JAHR [1] ## STATIONS_ID MESS_DATUM TEMPERATUR RELATIVE_FEUCHTE NIEDERSCHLAGSDA~ ## &lt;int&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 2019-09-23 09:50:00 18.8 66.7 0 ## 2 2564 2019-09-23 10:00:00 18.9 64.5 0 ## 3 2564 2019-09-23 10:10:00 19.5 61.3 0 ## 4 2564 2019-09-23 10:20:00 18.9 59.7 0 ## 5 2564 2019-09-23 10:30:00 19.4 58.2 0 ## 6 2564 2019-09-23 10:40:00 19.6 55.9 0 ## 7 2564 2019-09-23 10:50:00 19.8 55.8 0 ## 8 2564 2019-09-23 11:00:00 20.2 56.1 0 ## 9 2564 2019-09-23 11:10:00 20 57 0 ## 10 2564 2019-09-23 11:20:00 19.3 68.9 0 ## # ... with 5 more variables: NIEDERSCHLAGSHOEHE &lt;dbl&gt;, ## # NIEDERSCHLAGSINDIKATOR &lt;dbl&gt;, TAG &lt;int&gt;, MONAT &lt;dbl&gt;, JAHR &lt;dbl&gt; range(df_select_autumn$MESS_DATUM) ## [1] &quot;2019-09-23 09:50:00 UTC&quot; &quot;2019-12-22 05:10:00 UTC&quot; 5.5.2.1.4 Winter w_start_f &lt;- force_tz(int_start(int_season_winter),df_tz) w_end_f &lt;- force_tz(int_end(int_season_winter),df_tz) print(paste(&quot;Force TZ start/end&quot;, w_start_f, &quot;/&quot;, w_end_f)) ## [1] &quot;Force TZ start/end 2019-12-22 05:19:00 / 2020-03-20 04:48:00&quot; df_select_winter &lt;- df %&gt;% filter(MESS_DATUM &gt;= w_start_f &amp; MESS_DATUM &lt;= w_end_f) #df_group &lt;- df_sel %&gt;% group_by(STATIONS_ID , hour(MESS_DATUM) , day(MESS_DATUM) ) head(df_group, 10) range(df_select_winter$MESS_DATUM) ## [1] &quot;2019-12-22 05:20:00 UTC&quot; &quot;2020-03-20 04:40:00 UTC&quot; 5.6 Exploration - Analysis First we are going to calculate average temperatures for different standard intervalls like hours, days, weeks, months. We will visualise some data and eventually calculate new variables or aggregates for humdity, rainfall etc. as well, plus more insights, that might not be apparent at this stage. 5.6.1 Temperatur The first variable of interest should be “TEMPERATUR”. A quick visualisation delivers this picture: ggplot(df)+ geom_point(aes(MESS_DATUM, TEMPERATUR), colour = &quot;red&quot;, size = 0.1) This is a representation of all 52,704 observations of temperature and naturally appears quite crowded. However, a typical course of the seasons during a year can already be interpreted from this plot. Please note that the chart starts at the start of the observations (April 2019) and stretches over a year from there. Let’S try to get a clearer picture of the temperature variable during the course of the observed year. We need to form averages and aggregates to make visualisation more to the point and get less crowded pictures. New variables should represent other dimensions of date-time data. We can use lubridate functions to produce variables for hours, 24hdays, weeks, months and even possibly seasons. Assigning a year to every observation by creating a new column with lubridate function “year”: df &lt;- df %&gt;% mutate(JAHR = year(df$MESS_DATUM)) Assigning the specific month (number) to every observation by creating a new column with lubridate function “month” df &lt;- df %&gt;% mutate(MONAT = month(df$MESS_DATUM)) Assigning an Epiweeknr (EKW = Epi Kalender Woche) to every observation through a column with the function “epiweek”: df &lt;- df %&gt;% mutate(EKW = epiweek(df$MESS_DATUM)) Assigning a daynumber to every observation by creating a new column “JTAG” (Year day): df &lt;- df %&gt;% mutate(JTAG = yday(df$MESS_DATUM)) Assigning an hour to every observation by creating a new column “STUNDE”: df &lt;- df %&gt;% mutate(STUNDE = hour(df$MESS_DATUM)) Our data frame (df) has 12 variables now and thus we can filter the data by year, month, week, day and hour of the day. This gives us new possibilities to aggregate. The following GROUP_BY command creates a unique grouping per hour and then calculates the average temperature for every hour, then pastes this value into each observation per hour. df_group_Stunde &lt;- df %&gt;% group_by(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE) df_av_temp_Stunde &lt;- df_group_Stunde %&gt;% summarise(AVGTEMPH = mean(TEMPERATUR)) %&gt;% arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE) This newly created table has reduced the number of observations by the factor 6 to 8,784 and reveals the average temperature per hour. Let’s plot the data again as hourly temperature averages per day: ggplot(df_av_temp_Stunde)+ geom_point(aes(JTAG, AVGTEMPH), colour = &quot;red&quot;, size = 0.1) We can see that the data has become aggregated and that the temperature picture becomes somewhat more clear. Also the visualisation now stretches over the course of a whole calendar year from left to right. Let’s go one step further and shorten the data down to daily averages by: df_group_JTAG &lt;- df %&gt;% group_by(STATIONS_ID, JAHR, MONAT, EKW, JTAG) df_av_temp_JTAG &lt;- df_group_JTAG %&gt;% summarise(AVGTEMPD = mean(TEMPERATUR)) %&gt;% arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG) head(df_av_temp_JTAG, 10) ## # A tibble: 10 x 6 ## # Groups: STATIONS_ID, JAHR, MONAT, EKW [2] ## STATIONS_ID JAHR MONAT EKW JTAG AVGTEMPD ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2019 4 16 104 4.96 ## 2 2564 2019 4 16 105 5.58 ## 3 2564 2019 4 16 106 6.43 ## 4 2564 2019 4 16 107 8.17 ## 5 2564 2019 4 16 108 9.25 ## 6 2564 2019 4 16 109 9.63 ## 7 2564 2019 4 16 110 8.91 ## 8 2564 2019 4 17 111 11.4 ## 9 2564 2019 4 17 112 12.2 ## 10 2564 2019 4 17 113 12.6 This newly created table has reduced the number of observations by the factor 24 to 366 (the number of days in a leap year) and reveals the average temperature per day. Let’s plot the data again as dayly temperature averages over the whole period as a line: ggplot(df_av_temp_JTAG)+ geom_line(aes(JTAG, AVGTEMPD), colour = &quot;red&quot;, size = 0.5) The data starts to make even more visual sense. The key take-aways from this plot, next to the rather trivial finding of higher summer temperatures, is the rather high volatility that appears to be attached to daily average temperature throughout the year. This could be interpreted as the changes between high and low pressure weather systems that cross northern Germany. Let’s boil the data down to weekly temperature averages: df_group_EKW &lt;- df %&gt;% group_by(STATIONS_ID, JAHR, MONAT, EKW) df_av_temp_EKW &lt;- df_group_EKW %&gt;% summarise(AVGTEMPW = mean(TEMPERATUR)) %&gt;% arrange(STATIONS_ID, JAHR, MONAT, EKW) head(df_av_temp_EKW, 10) ## # A tibble: 10 x 5 ## # Groups: STATIONS_ID, JAHR, MONAT [3] ## STATIONS_ID JAHR MONAT EKW AVGTEMPW ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2019 4 16 7.56 ## 2 2564 2019 4 17 13.2 ## 3 2564 2019 4 18 10.4 ## 4 2564 2019 5 18 6.66 ## 5 2564 2019 5 19 8.43 ## 6 2564 2019 5 20 10.2 ## 7 2564 2019 5 21 13.4 ## 8 2564 2019 5 22 12.9 ## 9 2564 2019 6 22 16.3 ## 10 2564 2019 6 23 17.7 and plot again: ggplot(df_av_temp_EKW)+ geom_line(aes(EKW, AVGTEMPW), colour = &quot;red&quot;, size = 0.5) Even in weekyl aggregates of temperature averages the resulting visualisation still tells a story of volatility. Let’s look at monthly averages: df_group_MONAT &lt;- df %&gt;% group_by(STATIONS_ID, JAHR, MONAT) df_av_temp_MONAT &lt;- df_group_MONAT %&gt;% summarise(AVGTEMPM = mean(TEMPERATUR)) %&gt;% arrange(STATIONS_ID, JAHR, MONAT) head(df_av_temp_MONAT, 12) ## # A tibble: 12 x 4 ## # Groups: STATIONS_ID, JAHR [2] ## STATIONS_ID JAHR MONAT AVGTEMPM ## &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 2019 4 10.4 ## 2 2564 2019 5 10.6 ## 3 2564 2019 6 17.9 ## 4 2564 2019 7 17.3 ## 5 2564 2019 8 18.4 ## 6 2564 2019 9 14.0 ## 7 2564 2019 10 10.8 ## 8 2564 2019 11 6.16 ## 9 2564 2019 12 5.20 ## 10 2564 2020 1 5.74 ## 11 2564 2020 2 5.74 ## 12 2564 2020 3 5.58 and plot again: ggplot(df_av_temp_MONAT)+ geom_line(aes(MONAT, AVGTEMPM), colour = &quot;red&quot;, size = 1) Which is the kind of temperature curve we would expect to see in a north German location like Kiel with a clear pattern of 3 months of summer with higher average temperatures just below 20 degrees. It is interesting to note that (at least the first half of April 2020 is apparently much warmer on average than the second half of April in 2019. The dimension indicates a difference of about 2 degrees celsius, a significant difference. 5.6.1.1 Average temperature for a day Generally, no separate variables would have to be created to determine the average temperatures. The values can be gleaned differently from the data frame. df_avg1 &lt;- df_group %&gt;% summarise(avg = mean(TEMPERATUR)) %&gt;% arrange( STATIONS_ID , TAG , STUNDE) head(df_avg1, 10) ## # A tibble: 10 x 4 ## # Groups: STATIONS_ID, STUNDE [10] ## STATIONS_ID STUNDE TAG avg ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2564 0 1 5.45 ## 2 2564 1 1 5.32 ## 3 2564 2 1 5.22 ## 4 2564 3 1 5.3 ## 5 2564 4 1 5.53 ## 6 2564 5 1 5.58 ## 7 2564 6 1 5.35 ## 8 2564 7 1 5.9 ## 9 2564 8 1 6.18 ## 10 2564 9 1 6.5 5.6.2 Rainfall 5.6.2.1 Sum with condition The question here is, how long has ist rained during an hour/day? There are two different possible aproaches. One is without the binary indicator and the other uses it as a switch. Without indicator df_sum1 &lt;- df_group %&gt;% summarise(sum(NIEDERSCHLAGSDAUER)) %&gt;% arrange( STATIONS_ID , TAG, STUNDE) head(df_sum1, 24) ## # A tibble: 24 x 4 ## # Groups: STATIONS_ID, STUNDE [24] ## STATIONS_ID STUNDE TAG `sum(NIEDERSCHLAGSDAUER)` ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2564 0 1 0 ## 2 2564 1 1 0 ## 3 2564 2 1 0 ## 4 2564 3 1 0 ## 5 2564 4 1 0 ## 6 2564 5 1 0 ## 7 2564 6 1 0 ## 8 2564 7 1 0 ## 9 2564 8 1 0 ## 10 2564 9 1 0 ## # ... with 14 more rows With indicator and renamed column df_sum2 &lt;- df_group %&gt;% summarise(MENGE = sum(NIEDERSCHLAGSDAUER[NIEDERSCHLAGSINDIKATOR==1])) %&gt;% filter(MENGE&gt; 0) %&gt;% arrange( STATIONS_ID , TAG, STUNDE) head(df_sum2, 24) ## # A tibble: 7 x 4 ## # Groups: STATIONS_ID, STUNDE [7] ## STATIONS_ID STUNDE TAG MENGE ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 2564 11 1 2 ## 2 2564 12 1 3 ## 3 2564 13 1 23 ## 4 2564 14 1 14 ## 5 2564 15 1 10 ## 6 2564 18 1 16 ## 7 2564 23 1 1 Now we apply the knowledge with the indicator to the selection of autumn. First we examine the duration of rainfall in autumn. df_sum_autumn_group &lt;- df_group_autumn df_sum_autumn &lt;- df_sum_autumn_group %&gt;% summarise(DAUER = sum(NIEDERSCHLAGSDAUER[NIEDERSCHLAGSINDIKATOR==1])) %&gt;% filter(DAUER&gt;0) %&gt;% arrange( STATIONS_ID , JAHR, MONAT, TAG) head(df_sum_autumn, 10) ## # A tibble: 10 x 5 ## # Groups: STATIONS_ID, TAG, MONAT [10] ## STATIONS_ID TAG MONAT JAHR DAUER ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; ## 1 2564 24 9 2019 11 ## 2 2564 25 9 2019 593 ## 3 2564 26 9 2019 181 ## 4 2564 27 9 2019 392 ## 5 2564 28 9 2019 228 ## 6 2564 29 9 2019 1008 ## 7 2564 30 9 2019 354 ## 8 2564 1 10 2019 752 ## 9 2564 2 10 2019 146 ## 10 2564 3 10 2019 2 Now we examine the milliliter of rainfall. df_sum_autumn_liter_group &lt;- df_group_autumn df_sum_autumn_liter &lt;- df_sum_autumn_liter_group %&gt;% summarise(MENGE = sum(NIEDERSCHLAGSHOEHE[NIEDERSCHLAGSINDIKATOR==1])) %&gt;% filter(MENGE&gt;0) %&gt;% arrange( STATIONS_ID , JAHR, MONAT, TAG) head(df_sum_autumn_liter, 10) ## # A tibble: 10 x 5 ## # Groups: STATIONS_ID, TAG, MONAT [10] ## STATIONS_ID TAG MONAT JAHR MENGE ## &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2564 25 9 2019 0.71 ## 2 2564 26 9 2019 1.18 ## 3 2564 27 9 2019 13.3 ## 4 2564 28 9 2019 7.57 ## 5 2564 29 9 2019 14.6 ## 6 2564 30 9 2019 11.8 ## 7 2564 1 10 2019 15.4 ## 8 2564 2 10 2019 0.36 ## 9 2564 4 10 2019 0.1 ## 10 2564 5 10 2019 0.94 5.6.3 Humidity What we can say at this point is that the temperatures in the city of Kiel have a clear seasonal pattern, but remain highly volatile intra-day and inter-day. Weather however is not just defined as temperature, but humidity and precipitation play a role regarding our sensation and definition of weather as well. Let’s have one look at humidity the same way we looked at temperatures as hourly averages: df_av_humi_Stunde &lt;- df_group_Stunde %&gt;% summarise(AVGHUMI = mean(RELATIVE_FEUCHTE)) %&gt;% arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE) Let’s plot the data again as hourly humidity averages per day: ggplot(df_av_humi_Stunde)+ geom_point(aes(JTAG, AVGHUMI), colour = &quot;red&quot;, size = 0.1) This plot shows the dryer months during the summer as having a lot more hours with lower humidity values than the months that fall into autumn and winter. 5.7 Wrap up It was our objective to show and explain some of the more important features of the lubridate package. We can definitely say that handling this dataset in Base R would have been a lot more labourious. Lubridate has made working with this time series if not easy, but possible. This statement goes for the other used packages in this exploration (dplyr, ggplot, etc.) without which the whole task would have been quite tedious. "],
["forcats.html", "6 Categorical data: forcats 6.1 Introduction 6.2 General functions 6.3 Combine factors 6.4 Order of levels 6.5 Change the value of levels 6.6 Add or drop levels", " 6 Categorical data: forcats 6.1 Introduction This chapter is dedicated to the handling of categorical variables. This becomes important if information is to be presented in a non-alphabetical order or aggregated in a meaningful way. Within the R programming language, categorical variables are converted into a form that can be used for analysis using factors. While many Base-R functions automatically convert character vectors into factors, tidyverse requires an explicit treatment. The core-tidyverse provides the package forcats, which will be described here. Further information and exercises are available at the sources shown. Resources: Homepage Cheatsheet Chapter in R4DS Vignette In this chapter we use the demo data set diamonds from the ggplot2-package (more information) as well as a dataset olympic-games which shows the medal success of olympic athletes from 1896 to 2016. For the latter we focus on the summer games 1896 and the winter games 1924 for practical reasons. Before you start reading, you should have read the chapter Wrangling Data: dplyr or be familiar with this field. # The package tidyverse contains all libraries necessary for this chapter. library(tidyverse) # Importing the &quot;diamonds&quot; dataset. diamonds &lt;- read_csv(&quot;data/diamonds.csv&quot;) # Importing the &quot;olympic-games&quot; dataset. olympic &lt;- read_csv(&quot;data/olympic-games.csv&quot;) # Have a short look at the data. head(diamonds, 2) ## # A tibble: 2 x 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 # Have a short look at the data. head(olympic, 2) ## # A tibble: 2 x 12 ## game city sport discipline athlete country age height weight bmi sex ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 1992~ &lt;NA&gt; Bask~ Basketbal~ A Diji~ China 24 180 80 24.7 M ## 2 2012~ &lt;NA&gt; Judo Judo Men&#39;~ A Lamu~ China 23 170 60 20.8 M ## # ... with 1 more variable: medal &lt;chr&gt; 6.2 General functions 6.2.1 Create Basically two things are needed to create a factor: a vector which contains the values to be analyzed another vector which divides the values into levels As an example we will use the column clarity of the diamonds dataset. It is a categorical evaluation of the clarity of a diamond and a subset of the grades according to the Gemological Institute of America (GIA) grading system. The grades according to GIA read as follows: GIA grade Category Included in diamonds FL Flawless ✘ IF Internally Flawless ✔ VVS1 Very Very Slightly Included1 ✔ VVS2 Very Very Slightly Included2 ✔ VS1 Very Slightly Included1 ✔ VS2 Very Slightly Included2 ✔ SI1 Slightly Included1 ✔ SI2 Slightly Included2 ✔ I1 Included1 ✔ I2 Included2 ✘ I3 Included3 ✘ If you are interested in the distribution of the diamonds in this category, you could do this by using a suitable query: # Count the number of observations per clarity grade and plot the result. diamonds %&gt;% group_by(clarity) %&gt;% count() %&gt;% ggplot(aes(x = n, y = clarity)) + geom_col() A sorting of the x-axis, which follows the order of the grades as shown in the table above, is unfortunately not possible in this form. A workaround is to convert the column clarity into a factor, which allows us to evaluate the individual categories. For this purpose we first define a vector, which ranks the categories according to their grade (from bad to good): levels_clarity &lt;- c(&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;) In the next step we lay this newly created levels vector on the column clarity of our data set to create a factor. The function factor() is a base-R function. factor_clarity &lt;- factor(x = diamonds$clarity, levels = levels_clarity) The factor factor_clarity consists solely of the clarity column of the original diamonds data set. Another possibility to create a factor is to convert a vector using as_factor(). Here the levels are created automatically. Their order however depends on the appearance of the corresponding value in the source vector. as_factor(diamonds$clarity) %&gt;% levels() ## [1] &quot;SI2&quot; &quot;SI1&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;I1&quot; &quot;IF&quot; 6.2.2 Count values per level Now we repeat the analysis of the distribution within our dataset using the created factor with the levels in correct order. The function fct_count() returns the frequency of a categorical value within a factor. The order of the levels remains unchanged. fct_count(factor_clarity) %&gt;% # The definition of aesthetic mapping in this line is only used to label the axes. ggplot(aes(x = n, y = clarity)) + geom_col(aes(x = n, y = f)) It becomes obvious that the distribution can now be displayed in the desired order (by order of levels). Functions of the package forcats always start with the prefix fct_. 6.2.3 Inspect and set levels With the function levels() the levels of a factor can be both read and renamed. But be aware: this can only be used to change the names, not the order of the levels. The base-R function unclass() gives information about the internal memory structure of a factor. # Shows the original levels of our factor. factor_clarity %&gt;% levels() ## [1] &quot;I1&quot; &quot;SI2&quot; &quot;SI1&quot; &quot;VS2&quot; &quot;VS1&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot; # Shows the internal structure. factor_clarity[1:25] %&gt;% unclass() ## [1] 2 3 5 4 2 6 7 3 4 5 3 5 3 2 2 1 2 3 3 3 2 4 5 3 3 ## attr(,&quot;levels&quot;) ## [1] &quot;I1&quot; &quot;SI2&quot; &quot;SI1&quot; &quot;VS2&quot; &quot;VS1&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot; # Keep the order of levels, # but replace the technical GIA jargon with something more understandable. levels_clarity_c &lt;- c(&quot;c8&quot;, &quot;c7&quot;, &quot;c6&quot;, &quot;c5&quot;, &quot;c4&quot;, &quot;c3&quot;, &quot;c2&quot;, &quot;c1&quot;) levels(factor_clarity) &lt;- levels_clarity_c factor_clarity %&gt;% head(25) ## [1] c7 c6 c4 c5 c7 c3 c2 c6 c5 c4 c6 c4 c6 c7 c7 c8 c7 c6 c6 c6 c7 c5 c4 c6 c6 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 6.2.4 Inspect unique values The function fct_unique() can be used to output unique values of a factor. In contrast to the base-R function unique() the values are returned in the order of the levels and not in the order of their appearance. factor_clarity %&gt;% fct_unique() ## [1] c8 c7 c6 c5 c4 c3 c2 c1 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 factor_clarity %&gt;% unique() ## [1] c7 c6 c4 c5 c3 c2 c8 c1 ## Levels: c8 c7 c6 c5 c4 c3 c2 c1 6.3 Combine factors 6.3.1 Combine factors with different levels With the function fct_c() factors with different levels can be combined to one factor covering all levels. First we create two factors. The first one shows the amount of contested disciplines by several countries during the Summer Olympic Games 1896. The second one shows the amount of contested disciplines by countries during the Winter Olympic Games 1924. olympic_1896 &lt;- olympic %&gt;% filter(game == &quot;1896 Summer&quot;) # Create a factor using the default for the `levels` argument in `factor()`. countries_in_1896 &lt;- factor(olympic_1896$country) olympic_1924 &lt;- olympic %&gt;% filter(game == &quot;1924 Winter&quot;) # Create a factor using the default for the `levels` argument in `factor()`. countries_in_1924 &lt;- factor(olympic_1924$country) Note that the default option for the levels argument in factor() is a sorted set of the given values. As R sorts characters lexicographically, the corresponding levels are the countries sorted alphabetically from A to Z. The factors differ both in their stored values and in their levels. A plot makes this clear: countries_in_1896 %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country))+ geom_col(aes(x = n, y = f)) countries_in_1924 %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country))+ geom_col(aes(x = n, y = f)) Now we combine the factors with fct_c() and plot it: fct_c(countries_in_1896, countries_in_1924) %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country))+ geom_col(aes(x = n, y = f)) Both the underlying values and the levels were combined into one factor. 6.3.2 Standardise levels of various factors With the function fct_unify() the levels of different factors can be standardised. The resulting output factors will retain their values, but all will have the same levels. Please note, both input and output are in list form. factor_list &lt;- fct_unify(list(countries_in_1896, countries_in_1924)) # Plot the first output factor. factor_list[[1]] %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) # Plot the second output factor. factor_list[[2]] %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) In this case, the underlying values were left unchanged for both factors, but the levels were standardised. This is especially useful when comparing the values of two different factors. 6.4 Order of levels 6.4.1 Manual reordering of levels With the function fct_relevel() the levels of a factor can be manually reordered. In contrast, the function levels() only allows the renaming of factor levels. fct_relevel() also adjusts the order of the levels themselves by changing the way they are stored internally. An example should clarify this. # Have a look at the original levels. factor_clarity %&gt;% fct_count() ## # A tibble: 8 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 c8 741 ## 2 c7 9194 ## 3 c6 13065 ## 4 c5 12258 ## 5 c4 8171 ## 6 c3 5066 ## 7 c2 3655 ## 8 c1 1790 # Reorder levels according to `new_oder`. new_order &lt;- c(&quot;c1&quot;,&quot;c3&quot;,&quot;c5&quot;,&quot;c7&quot;,&quot;c2&quot;,&quot;c4&quot;,&quot;c6&quot;,&quot;c8&quot;) factor_clarity %&gt;% fct_relevel(new_order) %&gt;% fct_count() ## # A tibble: 8 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 c1 1790 ## 2 c3 5066 ## 3 c5 12258 ## 4 c7 9194 ## 5 c2 3655 ## 6 c4 8171 ## 7 c6 13065 ## 8 c8 741 Keep in mind, only the function fct_relevel() allows a correct manual releveling. However, levels() only allows to change the levels’ names. 6.4.2 Reordering by frequency Especially for plots it is often useful to sort the order of the levels by the frequency of the corresponding values. The function fct_infreq() allows exactly this. Plotting the unsorted factor causes poor readability. countries_in_1896 %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) The better approach is to sort the data before plotting. # Insert `fct_infreq()` to get the data ordered. countries_in_1896 %&gt;% fct_infreq() %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) 6.4.3 Reordering by appearance The package forcats contains the function fct_inorder() which enables the sorting of levels by their order of appearance in the data set. To make this clear, we look at the first 20 entries of the data set: olympic_1896$country %&gt;% head(20) ## [1] &quot;Greece&quot; &quot;Greece&quot; &quot;Greece&quot; &quot;Greece&quot; ## [5] &quot;Greece&quot; &quot;Greece&quot; &quot;Greece&quot; &quot;Greece&quot; ## [9] &quot;Greece&quot; &quot;Greece&quot; &quot;Great Britain&quot; &quot;Great Britain&quot; ## [13] &quot;Great Britain&quot; &quot;Switzerland&quot; &quot;Greece&quot; &quot;United States&quot; ## [17] &quot;United States&quot; &quot;Germany&quot; &quot;Germany&quot; &quot;Germany&quot; Create a factor with levels in the order of the appearance shown above: olympic_1896$country[1:20] %&gt;% fct_inorder() %&gt;% levels() ## [1] &quot;Greece&quot; &quot;Great Britain&quot; &quot;Switzerland&quot; &quot;United States&quot; ## [5] &quot;Germany&quot; In this example the same result can be obtained using the function as_factor(). However, the function fct_inorder() can also be applied to already existing factors. 6.4.4 Reverse level order The function fct_rev() reverses the existing order of the levels of a factor. First, have a look at the original order: # Insert `fct_infreq()` to get the data ordered. countries_in_1896 %&gt;% fct_infreq() %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) Now we reverse the order: # Insert `fct_infreq()` to get the data ordered. countries_in_1896 %&gt;% fct_infreq() %&gt;% # Insert `fct_rev()` to reverse this order. fct_rev() %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) 6.4.5 Shift levels The argument n in the function fct_shift() allows to shift the levels to the left/right for negative/positive integer values of n, wrapping around end. Thus, a value of n = 1L (n = -1L) would shift the order of the levels to the right (left) by one location. # Insert `fct_infreq()` to get the data ordered. countries_in_1896 %&gt;% fct_infreq() %&gt;% # Insert `fct_shift()` to shift levels. fct_shift(n = -1L) %&gt;% fct_count() %&gt;% ggplot(aes(x = `amount of disciplines`, y = country)) + geom_col(aes(x = n, y = f)) 6.4.6 Randomly shuffle levels The level of a factor can also be randomly shuffled using fct_shuffle(). The input argument can be either a factor or a character vector, whereas the output will be a factor. This is demonstrated using the factor countries_in_1896: # The original levels are sorted alphabetically. countries_in_1896 %&gt;% levels() ## [1] &quot;Australia&quot; &quot;Austria&quot; &quot;Denmark&quot; &quot;France&quot; ## [5] &quot;Germany&quot; &quot;Great Britain&quot; &quot;Greece&quot; &quot;Hungary&quot; ## [9] &quot;Italy&quot; &quot;Sweden&quot; &quot;Switzerland&quot; &quot;United States&quot; # The shuffled levels are randomly sorted. countries_in_1896 %&gt;% fct_shuffle() %&gt;% levels() ## [1] &quot;Great Britain&quot; &quot;Germany&quot; &quot;Sweden&quot; &quot;Greece&quot; ## [5] &quot;Italy&quot; &quot;France&quot; &quot;Australia&quot; &quot;Austria&quot; ## [9] &quot;Denmark&quot; &quot;Hungary&quot; &quot;United States&quot; &quot;Switzerland&quot; 6.4.7 Reordering levels by other variables The functions presented in this section bare great similarity to the fct_relevel() function introduced in the beginning of this chapter. fct_relevel() allows a direct manipulation of the levels by passing the new order to the levels keyword argument. At this point fct_reorder() and fct_reorder2() are different. Here, the levels are reordered according to the result of a function applied to one vector x in case of fct_reorder() and two vectors x and y in case of fct_reorder2(). Note: We want to point out a few things that have to be kept in mind when using these functions. The length of the factor f to be reordered has the be equal to the length of the vector(s) passed to the function. There will occur an error, if length(f) != length(x) or length(f) != length(y). In case the function returns the same value for two different elements of the vector(s), R will arrange the levels automatically. If there are multiple occurrences of an element, the level assigned to this element will be based on the result of the function for the first corresponding elements in x (and y). The example below is intended to unravel this behavior for fct_reorder(). We count the occurrences of countries in the olympic_1896 dataset and create a factor where the levels are sorted based on the number of occurrences. disciplines_per_country_in_1896 &lt;- olympic_1896 %&gt;% count(country) # Reorder the countries according to the number of disciplines. disciplines_per_country_in_1896$country %&gt;% fct_reorder(disciplines_per_country_in_1896$n, .desc = TRUE) ## [1] Australia Austria Denmark France Germany ## [6] Great Britain Greece Hungary Italy Sweden ## [11] Switzerland United States ## 12 Levels: Greece Germany United States France Great Britain ... Italy 6.5 Change the value of levels 6.5.1 Renaming the levels Let’s say you want to change the name of the levels (which also implies changing the corresponding value elements). fct_recode() allows to manually assign new names to certain levels without affecting the order of the levels or having an impact on levels that are not included in the function call. The diamonds$clarity column is an ideal example for the use of fct_recode(). For you conviniece we repeat some of the steps made in the beginning of this chapter: # Define the correct order of the levels in ascending order. levels_clarity &lt;- c(&quot;I1&quot;, &quot;SI2&quot;, &quot;SI1&quot;, &quot;VS2&quot;, &quot;VS1&quot;, &quot;VVS2&quot;, &quot;VVS1&quot;, &quot;IF&quot;) # Make `diamonds$clarity` a factor and assign the correctly ordered levels. factor_clarity &lt;- factor(x = diamonds$clarity, levels = levels_clarity) # Since the level notation is somewhat cryptic, we want to change it. renamed_factor_clarity &lt;- factor_clarity %&gt;% fct_recode(`Included 1 (worst)`=&quot;I1&quot;, `Slightly Included 2`=&quot;SI2&quot;, `Slightly Included 1`=&quot;SI1&quot;, `Very Slightly Included 2`=&quot;VS2&quot;, `Very Slightly Included 1`=&quot;VS1&quot;, `Very Very Slightly Included 2`=&quot;VVS2&quot;, `Very Very Slightly Included 1`=&quot;VVS1&quot;, `Internally Flawless (best)`=&quot;IF&quot;) # Show the renamed levels. renamed_factor_clarity %&gt;% levels() ## [1] &quot;Included 1 (worst)&quot; &quot;Slightly Included 2&quot; ## [3] &quot;Slightly Included 1&quot; &quot;Very Slightly Included 2&quot; ## [5] &quot;Very Slightly Included 1&quot; &quot;Very Very Slightly Included 2&quot; ## [7] &quot;Very Very Slightly Included 1&quot; &quot;Internally Flawless (best)&quot; Note, that this approach is in fact similar to the steps described in 5.2.3 Inspect and set levels. One could also combine multiple levels into one using fct_recode() as shown in the example below, where the numerical distinction within the GIA categories is dropped, that is Slightly Included 2 and Slightly Included 1 will be combined into the level Slightly Included, etc. # Since the level notation is somewhat cryptic, we want to change it # and drop the numerical distinction within each category. renamed_factor_clarity_2 &lt;- factor_clarity %&gt;% fct_recode(`Included (worst)`=&quot;I1&quot;, `Slightly Included`=&quot;SI2&quot;, `Slightly Included`=&quot;SI1&quot;, `Very Slightly Included`=&quot;VS2&quot;, `Very Slightly Included`=&quot;VS1&quot;, `Very Very Slightly Included`=&quot;VVS2&quot;, `Very Very Slightly Included`=&quot;VVS1&quot;, `Internally Flawless (best)`=&quot;IF&quot;) # Show the combined levels. renamed_factor_clarity_2 %&gt;% levels() ## [1] &quot;Included (worst)&quot; &quot;Slightly Included&quot; ## [3] &quot;Very Slightly Included&quot; &quot;Very Very Slightly Included&quot; ## [5] &quot;Internally Flawless (best)&quot; The resulting renamed_factor_clarity_2 now has only five distinct levels as compared to eight distinct levels in renamed_factor_clarity. Another scenario might be that we want to add either a pre- or a suffix or both to the current level or change the levels in some other general manner. To this end, the forcats package provides the fct_relabel() function. Its second argument (or its first argument in case of piping) is a function that must return a character vector. The countries_in_1896 factor variable is used to showcase how fct_relabel() works. add_prefix &lt;- function(input_char) { paste(&quot;Country:&quot;, input_char) } # The `add_prefix()` function is applied to each level # returning a character vector of the new labels. prefixed_countries_in_1896 &lt;- countries_in_1896 %&gt;% fct_relabel(add_prefix) # Show the combined levels. prefixed_countries_in_1896 %&gt;% levels() ## [1] &quot;Country: Australia&quot; &quot;Country: Austria&quot; &quot;Country: Denmark&quot; ## [4] &quot;Country: France&quot; &quot;Country: Germany&quot; &quot;Country: Great Britain&quot; ## [7] &quot;Country: Greece&quot; &quot;Country: Hungary&quot; &quot;Country: Italy&quot; ## [10] &quot;Country: Sweden&quot; &quot;Country: Switzerland&quot; &quot;Country: United States&quot; 6.5.2 Anonymize levels There might be situations, especially in the context of data privacy, where you want or need to anonymize your factor data. Assigning numeric IDs using fct_anon() is a good way to do this. It randomly assigns integer values (converted to characters) starting from 1 to the levels of a factor variable. The largest integer value to be assigned depends on the number of distinct levels in the variable. In the following example the countries_in_1896 factor variable with 12 distinct levels is randomly anonymized. # The function `fct_anon()` additionally allows to # define a prefix for the new random integer levels. countries_in_1896 %&gt;% fct_anon(prefix=&quot;id&quot;) %&gt;% head(25) ## [1] id09 id09 id09 id09 id09 id09 id09 id09 id09 id09 id04 id04 id04 id10 id09 ## [16] id02 id02 id05 id05 id05 id05 id05 id05 id05 id04 ## Levels: id01 id02 id03 id04 id05 id06 id07 id08 id09 id10 id11 id12 6.5.3 Collapse multiple levels into one The fct_collapse() function provides basically the same functionality as displayed in the 2nd example for fct_recode(). The syntax however is slightly different as the levels to be combined are specified in a single vector as shown in the example below. renamed_factor_clarity_3 &lt;- factor_clarity %&gt;% fct_collapse(`Included (worst)`=&quot;I1&quot;, `Slightly Included`= c(&quot;SI2&quot;, &quot;SI1&quot;), `Very Slightly Included`= c(&quot;VS2&quot;,&quot;VS1&quot;), `Very Very Slightly Included`= c(&quot;VVS2&quot;, &quot;VVS1&quot;), `Internally Flawless (best)`=&quot;IF&quot;) It should be noted, that fct_recode() could be easily replaced with fct_collapse() in the 2nd example for fct_recode() as fct_collapse() also works with single element character vectors. Replacing fct_collapse() with fct_recode() on the other hand is not possible, because fct_recode() cannot work with multiple element vectors such as c(\"SI2\", \"SI1\"). In this case, we recommend using fct_collapse() than fct_recode() for combining different levels. 6.5.4 Aggregate levels into a lump This topic is related to the rationale behind fct_collapse() described in the previous section. In contrast to specifying the levels to be combined explicitly, the forcats package also offers the possibility to lump levels together. “Lumping” is the act of combining several levels in order to create a single lump level called Other by default. There are several different functions that allow lumping based on different criteria. The functions which is probably easiest to understand is fct_lump_n() which takes an integer value n as the second (or the first argument in case of piping). It allows to preserve the n most/least frequent levels for n&gt;0 / n&lt;0. All other levels will be lumped together into one level that has the lowest order and is referred to as Other by default. Once again, we use data from the Summer Olympic Games in 1896 to give an example. # Only keep the three most frequent countries # and lump all other countries into `Other`. countries_in_1896 %&gt;% fct_lump_n(3) %&gt;% levels() ## [1] &quot;Germany&quot; &quot;Greece&quot; &quot;United States&quot; &quot;Other&quot; We just discovered that Germany was one of the countries competing in the most disciplines during the Summer Olympic Games 1896 although quite laboriously. Note, that we can however not conclude that most disciplines were competed by German athletes in 1896 as the levels are ordered alphabetically (except for Other which will always be last) and not by the number of disciplines per country. In fact, with 148 disciplines competed by Greek athletes, 94 by German and 27 by athletes from the US, Germany ranks second. To continue, we assume to be only interested in the countries whose athletes competed in at least 10 disciplines in the Summer Olympic Games 1896. For that purpose we use the fct_lump_min() function specifying the minimum of 10. # Only keep the countries whose athletes competed in at least 10 disciplines # and lump all other countries. countries_in_1896 %&gt;% fct_lump_min(10) %&gt;% levels() ## [1] &quot;Denmark&quot; &quot;France&quot; &quot;Germany&quot; &quot;Great Britain&quot; ## [5] &quot;Greece&quot; &quot;Hungary&quot; &quot;United States&quot; &quot;Other&quot; Now we know that there were only eight countries whose athletes competed in 10 disciplines or more. We could also be interested in relative rather than absolute number, say countries that provide at least a 10% share of the total number of disciplines. In 1896 there was a total number of 38 disciplines to compete in. To identify the desired set of countries, we lump proportionally using fcr_lump_prop(). # Only keep the countries whose athletes add up to # at least 10% of all competition records # and lump all other countries. countries_in_1896 %&gt;% fct_lump_prop(0.1) %&gt;% levels() ## [1] &quot;Germany&quot; &quot;Greece&quot; &quot;Other&quot; 6.5.5 Manually lump levels In the previous section, we combined levels into a lump called Other based on numerical criteria. In addition to this, we can also create our own lump using fct_other().In this function, we specify the levels we want to keep or drop as a vector to the keep or drop keyword argument. The following code demonstrates how to only keep the levels Denmark and Australia in the countries_in_1896 factor variable. # Only keep the levels `Denmark` and `Australia`. countries_in_1896 %&gt;% fct_other(keep=c(&quot;Denmark&quot;, &quot;Australia&quot;)) %&gt;% levels() ## [1] &quot;Australia&quot; &quot;Denmark&quot; &quot;Other&quot; # Keep all levels except for `Denmark` and `Australia`. countries_in_1896 %&gt;% fct_other(drop=c(&quot;Denmark&quot;, &quot;Australia&quot;)) %&gt;% levels() ## [1] &quot;Austria&quot; &quot;France&quot; &quot;Germany&quot; &quot;Great Britain&quot; ## [5] &quot;Greece&quot; &quot;Hungary&quot; &quot;Italy&quot; &quot;Sweden&quot; ## [9] &quot;Switzerland&quot; &quot;United States&quot; &quot;Other&quot; Note that the latter example is equivalent to using fct_collapse(Other=c(\"Denmark\", \"Australia\")). 6.6 Add or drop levels 6.6.1 Add levels In order to add levels to a factor variable we can use the fct_expand() function, which takes the level as an argument. For a more accurate demonstration, we switch to the diamonds dataset one more time as we want to use all GIA grades as levels now. # The additional GIA grades which do not exist in the diamonds dataset. additional_GIA_grades &lt;- c(&quot;FL&quot;, &quot;I2&quot;, &quot;I3&quot;) # Expand the original factor by the additional levels. expanded_factor_clarity &lt;- factor_clarity %&gt;% fct_expand(additional_GIA_grades) # Note that the new levels are appended. expanded_factor_clarity %&gt;% levels() ## [1] &quot;I1&quot; &quot;SI2&quot; &quot;SI1&quot; &quot;VS2&quot; &quot;VS1&quot; &quot;VVS2&quot; &quot;VVS1&quot; &quot;IF&quot; &quot;FL&quot; &quot;I2&quot; ## [11] &quot;I3&quot; Note that the additional levels get appended which is not necessarily what we want. Thus, an additional manual reordering of the levels as described in 5.4.1 Manual reordering of levels is required here: # All GIA grades in the correct order. all_GIA_grades &lt;- c(&quot;FL&quot;, &quot;IF&quot;, &quot;VVS1&quot;, &quot;VVS2&quot;, &quot;VS1&quot;, &quot;VS2&quot;, &quot;SI1&quot;, &quot;SI2&quot;, &quot;I1&quot;, &quot;I2&quot;, &quot;I3&quot;) # Reorder the levels of the expanded factor. reordered_expanded_factor_clarity &lt;- expanded_factor_clarity %&gt;% fct_relevel(all_GIA_grades) reordered_expanded_factor_clarity %&gt;% levels() ## [1] &quot;FL&quot; &quot;IF&quot; &quot;VVS1&quot; &quot;VVS2&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;SI1&quot; &quot;SI2&quot; &quot;I1&quot; &quot;I2&quot; ## [11] &quot;I3&quot; A less tedious approach would be to include the additional levels already in the definition of the factor variable using factor() as shown in the following. factor(x = diamonds$clarity, levels=all_GIA_grades) %&gt;% levels() ## [1] &quot;FL&quot; &quot;IF&quot; &quot;VVS1&quot; &quot;VVS2&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;SI1&quot; &quot;SI2&quot; &quot;I1&quot; &quot;I2&quot; ## [11] &quot;I3&quot; 6.6.2 Drop levels Unfortunately, we haven’t found any flawless diamond, so the level FL unused. To drop unused levels, the forcats package provides a function called fct_drop(). # Only drop the unused &quot;FL&quot; level. reordered_expanded_factor_clarity %&gt;% fct_drop(&quot;FL&quot;) %&gt;% levels() ## [1] &quot;IF&quot; &quot;VVS1&quot; &quot;VVS2&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;SI1&quot; &quot;SI2&quot; &quot;I1&quot; &quot;I2&quot; &quot;I3&quot; # Drop all unused levels when no argument is used. reordered_expanded_factor_clarity %&gt;% fct_drop() %&gt;% levels() ## [1] &quot;IF&quot; &quot;VVS1&quot; &quot;VVS2&quot; &quot;VS1&quot; &quot;VS2&quot; &quot;SI1&quot; &quot;SI2&quot; &quot;I1&quot; 6.6.3 Assign a level to NAs Usually when a vector contains NAs they are omitted in the conversion to a factor variable. The forcats package allows to explicitly assign a level to these values using the fct_explicit_na() function. To give an example, we return to the olympic_1896 dataset but we focus on the city column this time. # Convert the `medal` column into a factor. olympic_1896$medal %&gt;% factor() %&gt;% fct_count() ## # A tibble: 4 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Bronze 38 ## 2 Gold 62 ## 3 Silver 43 ## 4 &lt;NA&gt; 237 # Convert the `medal` column into a factor, replacing the NA values. olympic_1896$medal %&gt;% fct_explicit_na(&quot;No medal&quot;) %&gt;% fct_count() ## # A tibble: 4 x 2 ## f n ## &lt;fct&gt; &lt;int&gt; ## 1 Bronze 38 ## 2 Gold 62 ## 3 Silver 43 ## 4 No medal 237 "],
["stringr.html", "7 Character data: stringr 7.1 Introduction 7.2 Data Set for examples 7.3 Regular Expressions 7.4 Functions of stringR explained 7.5 Literature", " 7 Character data: stringr Resources: Homepage Book Chapter in R4DS Cheatsheet Vignette Into Vignette Regular Expressions Suggested data: recipes Advice: Many of the examples in the Vignettes just refer to vectors. How can we use stringr to create/modify character columns of a data frame? 7.1 Introduction 7.1.1 Use of stringR in the R environment StringR is a package that deals with measuring strings, search in strings and altering strings. StringR is thus mostly involved with data preprocessing, but it can also be used for text analysis (e.g. in text mining for determining the frequency of words). In the following, we will list most of the functions available in an encyclopedic style and describe their usage mostly within a single database, based on cooking recipes. To give an overall orientation of the functions, we want to present a short overview over some of the most commonly used functions. If you’re interested in counting the words within a text, go to detect matches and especially str_detect. str_subset could be interesting, if you have a collection of documents and you want to have a subset of these documents dealing with a certain topic. You can therefore choose a list of words related to the topic. Keep in mind that topic modelling might be better. The mutate-functions str_sub or str_replace can replace substrings or patterns in a text. This might be useful, if a word or expression is repeatedly used wrong and needs to be changed. Join several strings in order to create a long text can be done using str_c whereas the opposite, the splitting of string can be performed with the str_split function. Another useful tool in stringr are the str_sort and str_order functions. As their name indicates, they sort or order strings according to your conditions. Probably the use cases in real-life are rather technical, but hopefully the uses cases above provide some clarity of why stringR might be useful. 7.1.2 Types of data, which could be used with stringR The way you use the functions of stringR is dependent on which kind of data you want to analyze. The simplest example is having one word as an entry in every box, e.g. the in the column “job”, there might be an entry “cook”. Here stringR is best for formatting the words. StringR can be used on number entries as well. Regarding our example, there could also be more than one word per box, e.g. the tags for a computer game: “singleplayer, action, medieval”. stringR can help formatting these multiple entries into a desired form. Researching the package, we were under the impression that stringR is mostly designed for the purpose of managing and analyzing full texts or longer strings. We want to point out that full text analysis as done in text mining, e.g. the novel The Time Machine by H.G. Wells, doesn’t operate on a long text as a whole string. Instead, it’d rather tokenize the text first (Silge Robinson 2017). As a result every word is put in another row, as you can see here: ## # A tibble: 32,653 x 2 ## gutenberg_id word ## &lt;int&gt; &lt;chr&gt; ## 1 35 the ## 2 35 time ## 3 35 machine ## 4 35 by ## 5 35 h ## 6 35 g ## 7 35 wells ## 8 35 1898 ## 9 35 i ## 10 35 the ## # ... with 32,643 more rows This format will set restrictions on your ability to use the stringR functions, because stringR seems to be designed to operate on full texts and not on single worded strings. However, some stringR methods like str_detect are used frequently in text mining. 7.2 Data Set for examples As a data set for most of our examples we use the recipes data set, which contains recipes from various recipe websites from the USA. The dataset contains the recipe name, tags (such as “vegetarian”), the estimated time to cook it, the nutrition score, the ingredients, the steps needed to cook the recipe itself and a description of the recipe from the author. 7.3 Regular Expressions Regular expressions (regex) are used in almost every programming language. They are a way to normalize the syntax of strings. More importantly, they can not only be used to express a static word, but also to generally refer to defined group of words. For example, the set of words, which end on “r” contain the words “letter”, “bear”, “beer” etc. So, regular expressions are often used to match a set of words instead of a single word. For regular expressions, the following holds: For the regular expression “a”, you use the character “a” For all punctuation marks a backslash is put in front. E.g. “\\?” is the regular expression for the question mark: “?”. But to write “?” in R, you must use two backslashes: “\\\\?” For any digit, you can use the regular expression “ or also”[:digit:]“. For word characters, you use”\" or “[:alpha:]” as a regular expression. If you have finite list of alternatives, you can use the regular expression for one of those, like “[abc]”. So a word is just matched, if it either contains a, b or c. Another powerful regex might be “a[n]”, which indicates, that you just want to have those strings, which contain a n times. “a[2]” would match the string “banana”, because it contains two “n” s. You can group regular expressions with (). If you have the regex “(a|b)c”, you have three groups. First “(a|b)”, then “c” and the whole term as a group. Even if you have just set one pair of parenthesis. The intention of grouping a regex within a regex is to gain the possibility of calling each group by an assigned number. Moreover, some of the functions in stringr make use of the group structure, e.g. by providing a match for every group. Regex provide almost limitless possibilities for Data Scientists and are - when handled professionally - a powerful language for describing pattern in strings. Since giving credit to every regex would require the space of an own chapter in this book, we’d like to redirect you to the stringr cheat sheet of stringr. Please visit http://edrub.in/CheatSheets/cheatSheetStringr.pdf for more information about regex. Keep in mind that it might be difficult to find regular expressions, which define the members of the set you want to match. Take a look at the string “love”. If you want to match every conjugation or word related to “love”, like “loved”, “lover” etc, you’ll need a regular expression to define this set. If you use the regular expression love\\\\w\" , most matches will be given back correctly. For example, when matching “loved”, “loved” is given back. However, if matching “lovely” just “lovel” is given back, because you only have accounted for one additional character. Always look out for caveats like this in regular expressions. 7.4 Functions of stringR explained 7.4.1 Manage lengths 7.4.1.1 str_length str_length(string) shows the number of characters (!not words) in the given argument string. This includes both white spaces and punctuation marks. This could be used to compare the length of two strings, especially if they have to have the same length. This could also be used to determine the frequency of characters in a string. Example To show that punctuation marks and white spaces are also counted, we use here a simple example. str_length(&quot;Hello, world!&quot;) ## [1] 13 In the above string there are 13 characters and 10 word characters. If you just want to get just the amount of word characters, you could subset the string by matching every character, which is a word character. 7.4.1.2 str_pad str_pad(string, width, side = c(“left”, “right”,“both”), pad = \" \") adds characters either to the left or right of a string (as a default white spaces). The argument width states how long the string should be after the adding of the additional characters. Both as a parameter for the side means that the added characters are split between the left and the right side of the string. With the pad argument any other character can be added. The idea is to adjust strings of different length to the same length in order to match them in a subsequent step. E.g. when the comparison of the numbers 000247 with 247 should result in a match, we add 0s in front of the second number. Example In our dataset, the id consists sometimes of 5 or 6 digits, as you can see here: ## [1] 144176 288190 174243 186363 231008 225599 70689 218370 90181 308152 We want to adjust all of the numbers to the same length. So, we add a zero in front to those 5-digit numbers. ## [1] &quot;144176&quot; &quot;288190&quot; &quot;174243&quot; &quot;186363&quot; &quot;231008&quot; &quot;225599&quot; &quot;070689&quot; &quot;218370&quot; ## [9] &quot;090181&quot; &quot;308152&quot; As we can see, some ids have now a 0 in front. These were 5-digit numbers before. This works also, if id is of numeric type in r, but the type is then changed to character. As you can see here, the numbers above don’t have quotation marks, while the ones below have some. 7.4.1.3 str_trunc str_trunc(string, width, side = c(“right”, “left”, “center”), ellipsis = “…”) shortens the string to the given argument width. The side, on which the string is cut off is determined by the side argument. Example We could use the function to show more text from the description column than can be seen in the view(recipes) tab, to get a better impression, of what these texts are about. But we might not need the full text. We just want the first 50 characters, so we cut off to the right. head(str_trunc(recipes$description, width = 50, side = &quot;right&quot;),5) ## [1] &quot;from oregonlive.com. i loved the beef lettuce ...&quot; ## [2] &quot;this is just a recipe i came up with when i was...&quot; ## [3] &quot;i just love this recipe. it&#39;s quick, easy to ma...&quot; ## [4] &quot;this is another of our family favorites that ma...&quot; ## [5] &quot;finally comfort food for us vegetarians!! post...&quot; recipes %&gt;% mutate(description_short = str_trunc(description, width = 50, side = &quot;right&quot;)) %&gt;% select(name, description_short) %&gt;% head() ## # A tibble: 6 x 2 ## name description_short ## &lt;chr&gt; &lt;chr&gt; ## 1 chinese spicy beef lettuce wraps from oregonlive.com. i loved the beef ~ ## 2 peanut butter and dark fudge brownie~ this is just a recipe i came up with wh~ ## 3 dorito casserole i just love this recipe. it&#39;s quick, ea~ ## 4 pink lady squares this is another of our family favorites~ ## 5 vegetable pot pie pies finally comfort food for us vegetarians~ ## 6 artichoke gratinata this is a giada delaurentis recipe that~ 7.4.1.4 str_trim str_trim(string, side = c(“both”, “left”, “right”)) trims whitespaces from the start or end or both. Example str_trim(&quot; Hello, world! &quot;, side = &quot;left&quot;) ## [1] &quot;Hello, world! &quot; As you can see, the whitespaces on the left are cut out, while the whitespaces on the right are still there. For deleting all the white spaces in a larger string, you could also use a regular expression and match all not white spaces by using “\\S”. 7.4.2 Subset Strings 7.4.2.1 str_sub str_sub(string, start=1L, end = -1L) can be used to extract substrings of a string. The first argument requires text and as 2nd and 3rd parameter the start and end index of the substring must be given. This can also be used to delete characters at the beginning and end of a string by extracting a version of the string without the beginning and end characters. Example In our data we have the nutrition given as a character string in the following format ## [1] &quot;[267.4, 18.0, 15.0, 28.0, 42.0, 20.0, 5.0]&quot; This whole term is a string, which we want to split into a list. First, we have to delete the “[\" in the beginning of the string and \"]” at the end. Here it is useful, that We can use the str_sub also to eliminate unwanted signs at the beginning and the end of a string. str_sub(recipes$nutrition[1], 2,str_length(recipes$nutrition[1])-1) ## [1] &quot;267.4, 18.0, 15.0, 28.0, 42.0, 20.0, 5.0&quot; Then we can use the strsplit() function from base R to split the string into a list of strings by using “,” as the seperator. We don’t have to care for the brackets additionally then. 7.4.2.2 str_subset str_subset(string, pattern) gives back strings that contain a pattern match. The function looks for all strings with the argument pattern in it and returns all the fitting strings as a (possibly) smaller set of strings. Example To give an example, we look at the description of a recipe, which looks like the following: ## [1] &quot;from oregonlive.com. i loved the beef lettuce wraps from a restaurant in montauk, ny called east by northeast. this was the closest i have found to recreating it with a few minor adjustments from me.&quot; Some of the descriptions contain the source of the recipe. So in this case, we want all the recipes, which originate from oregonlive.com str_subset(recipes$description, &quot;oregonlive.com&quot;) ## [1] &quot;from oregonlive.com. i loved the beef lettuce wraps from a restaurant in montauk, ny called east by northeast. this was the closest i have found to recreating it with a few minor adjustments from me.&quot; ## [2] &quot;this is a recipe in transition. it was made after a conversation with a friend. there is no place that i have been able to find a recipe for bacon lo mein - this was the closest i came on the internet. originally, i found this on oregonlive.com, tuesday, nov. 29, 2005 from philip jones, fort atkinson, wis. however, have tweaked it and now it&#39;s mine bad or good! here&#39;s to ya&#39;.update: 01/22/2010 - to the reviewer who gave it a thumbs up - did you mean to omit the stars? after all - i got a glowing report! ;)&quot; As you can see, the set of strings has been reduced just to two strings. These are those, which contain “oregon.live” 7.4.2.3 str_extract str_extract(string, pattern) returns the first pattern match found in each string as a vector. If you simply give in a word as an argument for pattern, then the first match is either the word itself or the function gives NA out, if there is no match. If you give in a set of strings, the result is a vector with just the two entries mentioned before. Since this just gives the information, if a string contains a word or not, the str_detect function would be more suitable for the task. The function can also be used with a vector of strings as an argument for pattern. Then the function gives back the first string of the vector, which has been found in the string of the first argument, which shall be searched. str_detect would just indicate that one of the strings in the vector is true, but not which of them. If you don’t simply want the first match, then you can use also str.extract.all(). The output is a list of vectors. Example 1 We look at the common words love and recipe in the description. The result shows, in which decriptions which word of these comes first or gives back NA if the pattern isn’t found at all in the string. head(str_extract(recipes$description, &quot;(love\\\\w|recipe\\\\w)&quot;),60) ## [1] &quot;loved&quot; NA NA NA NA NA NA ## [8] NA NA NA NA &quot;recipes&quot; NA NA ## [15] NA NA NA NA NA NA &quot;recipes&quot; ## [22] NA NA NA NA NA &quot;recipes&quot; NA ## [29] NA NA NA NA NA &quot;loves&quot; NA ## [36] NA &quot;recipel&quot; NA NA NA NA NA ## [43] NA NA NA &quot;recipes&quot; NA NA NA ## [50] NA NA NA NA &quot;recipes&quot; &quot;lovel&quot; NA ## [57] NA NA NA NA This example also shows, that you have to be careful with regular expressions. The 55rd entry “lovel” comes from “lovely”. Maybe you would rather have “lovely” returned. Example 2 One could also look at the ingredients and check, which recipes contain milk and eggs. Since we want to show that both are present in the dish or just one of these ingredients, we use str_extract_all head(str_extract_all(recipes$ingredients, &quot;milk|eggs&quot;),5) ## [[1]] ## character(0) ## ## [[2]] ## [1] &quot;eggs&quot; &quot;milk&quot; ## ## [[3]] ## character(0) ## ## [[4]] ## [1] &quot;milk&quot; ## ## [[5]] ## [1] &quot;milk&quot; As you can see the second recipe contains both eggs and milk, the 4th and 5th recipe contain just milk, but no eggs. 7.4.2.4 str_match str_match(string, pattern) returns the first pattern match found in each string, as a matrix with a column for each () group in pattern. This is based on groups from regular expressions. Strings can be grouped via brackets (). Everything within this bracket belongs to the same group. E.g. in the regex (a|b)(c|d) the regular expression a|b is one group. because they’re in the same bracket. In this function if a match of a is detected, this is a match for group 1 and thus “a” is put into column 2. The first column is always reserved for the whole expression. E.g. if the string “ac” is found, there will be an entry “ac” in the first column, “a” in the second column, because this is the letter matched for the first group. In the third column “c” will be matched, because “c” belongs to the second group. As with str_extract, here also exists a str_match_all function to get all the matches and not just the first. Example Here we want to match the regular expression (love\\w|recipe\\w).+(beef). This means “love” and “recipe” form one group and beef forms the other. The whole expression as such is also a group. head(str_match(recipes$description, &quot;(love\\\\w|recipe\\\\w).+(beef)&quot;),5) ## [,1] [,2] [,3] ## [1,] &quot;loved the beef&quot; &quot;loved&quot; &quot;beef&quot; ## [2,] NA NA NA ## [3,] NA NA NA ## [4,] NA NA NA ## [5,] NA NA NA As you can see, in the first column there is the entry beloging to the whole regular expression as a group. The string contains “love” as well as “beef”. In the second column, the string matched from the first group is “love”, in the third column the string matched from the second group is “beef”. 7.4.3 Detect matches 7.4.3.1 str_detect str_detect(string, pattern) detects the presence of a pattern match in a string. The result is simply TRUE and FALSE Example Here we search for love and recipe in the description of every recipe. head(str_detect(recipes$description, &quot;(love\\\\w|recipe\\\\w)&quot;),100) ## [1] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [13] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE ## [25] FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE FALSE FALSE ## [37] TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE NA TRUE FALSE FALSE ## [49] FALSE FALSE FALSE FALSE FALSE TRUE TRUE FALSE FALSE FALSE FALSE FALSE ## [61] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [73] FALSE FALSE FALSE FALSE TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE ## [85] FALSE FALSE FALSE NA TRUE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [97] FALSE FALSE FALSE TRUE In contrast to extract, we just see, that one of the strings is in a certain string or not, but not which one of those. 7.4.3.2 str_which str_which(string, pattern) finds the indices of strings that contain a pattern match. The output is a vector with these indices. Example Again we search for “love” and “recipe” in the description column. head(str_which(recipes$description, &quot;(love\\\\w|recipe\\\\w)&quot;),100) ## [1] 1 12 21 27 34 37 46 54 55 77 89 96 100 115 125 126 127 146 ## [19] 152 154 166 169 174 180 191 192 193 200 224 227 234 239 246 276 282 286 ## [37] 302 309 311 325 333 337 345 346 352 354 367 389 392 402 405 410 417 427 ## [55] 431 435 440 442 457 463 465 473 483 487 497 509 516 518 522 523 538 550 ## [73] 555 560 564 573 575 596 604 611 612 620 639 649 666 670 677 695 712 724 ## [91] 725 732 734 746 768 770 779 780 788 798 In these rows there are descriptions, which contain either the word love or recipe. 7.4.3.3 str_count str_count(string, pattern) counts the number of matches in a string. Example “i” occurs very often in the descriptions of the recipes. Let’s see how often per description. As a regular expression we have chosen \" i “, that is an i surrounded by white spaces. This means in sentences like”I ….\" the i is not matched. head(str_count(recipes$description, &quot; i &quot;),100) ## [1] 2 2 0 1 0 1 0 0 2 0 1 0 0 1 0 1 0 0 1 1 0 0 0 1 2 ## [26] 2 1 8 0 0 0 6 0 0 0 0 3 0 0 0 0 0 0 0 NA 0 0 0 0 2 ## [51] 2 0 1 4 0 2 2 0 1 0 1 0 1 0 0 1 0 1 0 2 0 0 0 0 2 ## [76] 0 2 0 1 1 2 0 0 0 1 0 0 NA 2 0 0 0 0 0 0 1 2 0 0 0 7.4.3.4 str_locate str_locate(string, pattern) locates the positions of the first pattern match within a string. head(str_locate(recipes$description, &quot; i &quot;),10) ## start end ## [1,] 22 24 ## [2,] 22 24 ## [3,] NA NA ## [4,] 175 177 ## [5,] NA NA ## [6,] 40 42 ## [7,] NA NA ## [8,] NA NA ## [9,] 112 114 ## [10,] NA NA If you look up the first description string ## [1] &quot;from oregonlive.com. i loved the beef lettuce wraps from a restaurant in montauk, ny called east by northeast. this was the closest i have found to recreating it with a few minor adjustments from me.&quot; Then you can see, that the first \" i \" is indeed from position 22 to 24. The length of 3 (24-22+1) come from 2 whitespaces and the i. 7.4.4 Mutate Strings In this part we will use Italy, a subset of “recipes” that is exclusively containing recipes mentioning “pizza”. 7.4.4.1 str_replace (and replace_all) The replace functions of stringr are used to match patterns and replace them with new strings. Example Italy &lt;- Italy %&gt;% mutate(name1 = str_replace(Italy$name, &quot;pizza&quot;, &quot;fluffy pizza&quot;)) For seo (search engine optimization) we will add the popular word “fluffy” to all our pizza recipes in the name column. In order to replace several matches, use str_replace_all and create a vector containing multiple conditions. Italy &lt;- Italy %&gt;% mutate(name2 = str_replace_all(Italy$name, c(&quot;pizza&quot; = &quot;fluffy pizza&quot;, &quot;easy&quot; = &quot;easy peasy&quot;))) 7.4.4.1.1 str_to_lower We can use str_to_ to change the case of text strings. They are intuitive: str_to_lower will change text to lower case, str_to_upper will change text to upper case, str_to_title will change first letter of a string into a capital letter. … Example p &lt;- c(&quot;i&quot;,&quot;like&quot;,&quot;pizza&quot;,&quot;and&quot;,&quot;i don&#39;t share pizzza&quot;) str_to_upper(p) ## [1] &quot;I&quot; &quot;LIKE&quot; &quot;PIZZA&quot; ## [4] &quot;AND&quot; &quot;I DON&#39;T SHARE PIZZZA&quot; str_to_sentence(p) ## [1] &quot;I&quot; &quot;Like&quot; &quot;Pizza&quot; ## [4] &quot;And&quot; &quot;I don&#39;t share pizzza&quot; p &lt;- str_c(p, collapse = &quot; &quot;) #make it one sentence (see str_c) str_to_sentence(p) ## [1] &quot;I like pizza and i don&#39;t share pizzza&quot; Attention: Locales will apply language conventions when changing the case. As you know, capitalization is used different in German and English. Above, the default would be locale = “en” but you can change it into every other language. The code for your language can be found in Wikipedia - List of ISO 639-1 codes. Locales also come in handy for the str_order or str_sort fuction. str_sort will sort a list of string vectors by alphabet (locale will determine wich alphabet is applied) str_sort(Italy$name1[1:10], decreasing = FALSE, na_last = TRUE, locale = &quot;en&quot;, numeric = FALSE) ## [1] &quot;fluffy pizza tomato sauce&quot; ## [2] &quot;karen s slow cooker fluffy pizza chicken&quot; ## [3] &quot;macaroni fluffy pizza casserole&quot; ## [4] &quot;mini fluffy pizza muffins&quot; ## [5] &quot;parmesan spinach fluffy pizza&quot; ## [6] &quot;pizzeria style sausage fluffy pizza&quot; ## [7] &quot;quick bread fluffy pizza&quot; ## [8] &quot;strawberry fluffy pizza&quot; ## [9] &quot;three cheese fluffy pizza with truffle oil&quot; ## [10] &quot;yeast free fluffy pizza with fresh basil and tomatoes&quot; 7.4.5 Join and Split 7.4.5.1 str_c If you want to merge several strings into one, the str_c function is what you need. It is simple: either you join two or more single strings using the separate argument to choose a separation sign or you join a vector of strings by having the argument collapse control how the strings are separated. Example In our Pizza example you can first see the join of the first few names into one string separated by “,” and then a vector that is joint into one string using the words “and then” as a form of separation. str_c(Italy$name2[1],Italy$name2[2], Italy$name2[3], sep = &quot;, &quot;) ## [1] &quot;three cheese fluffy pizza with truffle oil, karen s slow cooker fluffy pizza chicken, yeast free fluffy pizza with fresh basil and tomatoes&quot; str_c(Italy$name2[1:3], collapse = &quot; and then &quot;) ## [1] &quot;three cheese fluffy pizza with truffle oil and then karen s slow cooker fluffy pizza chicken and then yeast free fluffy pizza with fresh basil and tomatoes&quot; 7.4.5.2 str_split The str_split function will split a vector of strings into a list of substrings. The split is made where there is a match with a pattern of your choice. Example In our example we will split the column step containing step by step instructions into single substrings. As the steps currently have the format of a list, we can use the comma as a pattern to separate the strings. Italy$steps %&gt;% str_split(&quot;&#39;, &#39;&quot;) %&gt;% head(2) ## [[1]] ## [1] &quot;[&#39;follow heating directions for crust&quot; ## [2] &quot;sprinkle mozzarella and parmesan cheese evenly on crust&quot; ## [3] &quot;dollop the ricotta cheese to your liking on top&quot; ## [4] &quot;bake pizza until cheese get bubbly&quot; ## [5] &quot;drizzle truffle oil over cooked pizza&quot; ## [6] &quot;enjoy&#39;]&quot; ## ## [[2]] ## [1] &quot;[&#39;place chicken , onion , bell pepper and celery in a slow cooker&quot; ## [2] &quot;in a medium bowl combine the tomato soup , cream of mushroom soup , tomato paste , water , parsley , oregano , basil , salt and pepper&quot; ## [3] &quot;mix well and pour mixture over chicken and vegetables in slow cooker&quot; ## [4] &quot;stir to coat and add bay leaf&quot; ## [5] &quot;cook on low setting for 8 hours , until chicken and vegetables are tender&#39;]&quot; If you’d rather have the subset of strings in a matrix, you can use str_split_fixed and choose n for the number of columns in the matrix. 7.4.5.3 str_glue Example pandas &lt;- c(&quot;Carl&quot;,&quot;Jakob&quot;,&quot;Jakob&quot;,&quot;Markus&quot;,&quot;Robert&quot;) str_glue(&quot;Hey {pandas}! I&#39;d recommend the recipe &#39;{str_to_title(Italy$name2[1:5])}&#39; for you.&quot; ) ## Hey Carl! I&#39;d recommend the recipe &#39;Three Cheese Fluffy Pizza With Truffle Oil&#39; for you. ## Hey Jakob! I&#39;d recommend the recipe &#39;Karen S Slow Cooker Fluffy Pizza Chicken&#39; for you. ## Hey Jakob! I&#39;d recommend the recipe &#39;Yeast Free Fluffy Pizza With Fresh Basil And Tomatoes&#39; for you. ## Hey Markus! I&#39;d recommend the recipe &#39;Parmesan Spinach Fluffy Pizza&#39; for you. ## Hey Robert! I&#39;d recommend the recipe &#39;Macaroni Fluffy Pizza Casserole&#39; for you. As you can see, str_glue literally glues strings together to create a new string. Expressions must be written in {}, everything else is up to you tough: you can use data frames, lists and regex. Useful for customized mailing, distribution of tasks ect. 7.5 Literature Silge, J., &amp; Robinson, D. (2017). Text mining with R: A tidy approach. \" O’Reilly Media, Inc.\". "],
["data-table.html", "8 High performance computing: data.table 8.1 Motivation 8.2 Data exploration 8.3 Runtime comparision 8.4 Further resources", " 8 High performance computing: data.table Okay, let’s get started with this chapter on high performance computing using the R package data.table. ## [1] &quot;Hello data.table&quot; If data.table is not yet installed, we install it via: install.packages(&quot;data.table&quot; , repos=&quot;http://cran.us.r-project.org&quot;) We load data.table package and - to demonstrate the differences - also the tidyverse library. library(tidyverse) library(data.table) Data.table objects are similar to a data.frame object but with some extensions. Let us first load some data with the data.table file reader fread(), which is optimised for performance and creates a data.table object. inpOlympic &lt;- if (file.exists(&quot;data/olympic-games.csv&quot;)) { &quot;data/olympic-games.csv&quot; } inpSpoti &lt;- if (file.exists(&quot;data/spotify_charts_germany.csv&quot;)) { &quot;data/spotify_charts_germany.csv&quot; } olympics &lt;- fread(inpOlympic) spotify &lt;- fread(inpSpoti) 8.1 Motivation The package data.table provides an enhanced data frame object called (big surprise: ) data.table and a different type of syntax for common types of data manipulations. In this sense it is similar to the tidyverse (especially the dplyr part of it). A data.table object has the following form DT[i,j,by] where i represent the rows, j the columns and by the grouping argument. class(olympics) # We see that a data.table inherits properties of data.frames ## [1] &quot;data.table&quot; &quot;data.frame&quot; # and we can easily cast an data.frame object to data.table matrix &lt;- data.frame(c(1,2,3),c(4,5,6),c(7,8,9)) class(matrix) ## [1] &quot;data.frame&quot; setDT(matrix) class(matrix) ## [1] &quot;data.table&quot; &quot;data.frame&quot; So what are the main differences between data.table and tidyverse? Why or when would we want to use one or the other Performance: data.tables are heavily optimised for speed. If performance is crucial, then data.tables might be the better choice. Syntax: data.table syntax is concise while tidyverse syntax is more expressive. Code sequences that span multiple lines are easier to read in case of tidyverse syntax due to the pipe (%&gt;%) operator. Overall, the choice is a matter of personal preferences here. Mutability: data.tables are mutable, i.e. they can be changed in place (by reference). If this is a desired feature, then we might opt for data.tables. Quoted and unquoted names: in data.tables we usually have the option to use either quoted column names (which makes writing functions easier) or unquoted column names (which is convenient for exploration). In dplyr the default is unquoted column names and it is a bit trickier to use quoted column names. Before we go into the details, let’s start with two motivating examples of differences between data.table and tidyverse syntax. First, we want to know the 5 sports with the highest average weight of players in the 2016 summer olympic games. # data.table syntax is short olympics[game==&quot;2016 Summer&quot;, .(weight = mean(weight, na.rm=T)), sport][order(-weight)][1:5] ## sport weight ## 1: Basketball 87.86786 ## 2: Water Polo 85.44961 ## 3: Handball 83.26912 ## 4: Volleyball 80.29329 ## 5: Rowing 79.87226 # tidyverse syntax is more expressive and readable olympics %&gt;% filter(game==&quot;2016 Summer&quot;) %&gt;% group_by(sport) %&gt;% summarise(weight = mean(weight, na.rm=TRUE)) %&gt;% arrange(-weight) %&gt;% head() Next, we want to change an existing column: # data.tables are mutable objects, i.e. changes happen in-place olympics[,age:=as.numeric(age)] # age is coerced from character to numeric # tidyverse&#39;s tibbles are immutable, i.e. we need to reassign to make permanent changes to the data olympics &lt;- olympics %&gt;% mutate(age_numeric =as.numeric(age)) 8.2 Data exploration As mentioned before the datatype data.table has the following structure: dt[i,j,by], where i conditon on the rows, j select the colums and by defines the groups by which results are aggregated. We’ll start by setting conditions on the rows. 8.2.1 Subsetting rows We can condition on the rows one time, which gives us all swimmers. again, for repetition the code tidyverse syntax swimmers &lt;- olympics %&gt;% filter(sport == &quot;Swimming&quot;) and in data.table syntax swimmers &lt;- olympics[sport == &quot;Swimming&quot;] olympics[sport==&quot;Swimming&quot;] # or without assinging operator ## game city sport discipline ## 1: 1952 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 400 metres Freestyle ## 2: 1912 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 200 metres Breaststroke ## 3: 1912 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 400 metres Breaststroke ## 4: 1920 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 200 metres Breaststroke ## 5: 1920 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 400 metres Breaststroke ## --- ## 23191: 2000 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 4 x 100 metres Medley Relay ## 23192: 2004 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 4 x 100 metres Freestyle Relay ## 23193: 1924 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 200 metres Breaststroke ## 23194: 1980 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 100 metres Butterfly ## 23195: 1980 Summer &lt;NA&gt; Swimming Swimming Men&#39;s 200 metres Butterfly ## athlete country age height weight bmi ## 1: Einar Ferdinand &quot;&quot;Einari&quot;&quot; Aalto Finland 26 NA NA NA ## 2: Arvo Ossian Aaltonen Finland 22 NA NA NA ## 3: Arvo Ossian Aaltonen Finland 22 NA NA NA ## 4: Arvo Ossian Aaltonen Finland 30 NA NA NA ## 5: Arvo Ossian Aaltonen Finland 30 NA NA NA ## --- ## 23191: Klaas Erik &quot;&quot;Klaas-Erik&quot;&quot; Zwering Netherlands 19 189 80 22.39579 ## 23192: Klaas Erik &quot;&quot;Klaas-Erik&quot;&quot; Zwering Netherlands 23 189 80 22.39579 ## 23193: Marius Edmund Zwiller France 18 NA NA NA ## 23194: Bogusaw Stanisaw Zychowicz Poland 19 189 80 22.39579 ## 23195: Bogusaw Stanisaw Zychowicz Poland 19 189 80 22.39579 ## sex medal ## 1: M &lt;NA&gt; ## 2: M &lt;NA&gt; ## 3: M &lt;NA&gt; ## 4: M Bronze ## 5: M Bronze ## --- ## 23191: M &lt;NA&gt; ## 23192: M Silver ## 23193: M &lt;NA&gt; ## 23194: M &lt;NA&gt; ## 23195: M &lt;NA&gt; And two or more times with the ‘&amp;’ operator (‘|’ operator for the logical or),for example all gold medal winners in swimming. swimmers_gold &lt;- olympics[sport == &quot;Swimming&quot; &amp; medal == &quot;Gold&quot;] swimmers_gold_without_doping &lt;- swimmers_gold[!country==&quot;Russia&quot; &amp; country!= &quot;China&quot;] #the `!` operator negates the logical expression The i argument also allows for regular expressions Michael_Phelps &lt;- olympics[sport==&quot;Swimming&quot; &amp; athlete %like% &quot;Phelps&quot;] The columns can be treated like variables so the expression olympics$sport is not necessary here but would work too. In additon we can select for the row numbers: tidyverse: var &lt;- swimmers %&gt;% slice(1:5) data.table var &lt;- swimmers[1:5] This is especially useful after ordering the data: height &lt;- swimmers[order(height)] or in decreasing order( here we pick the first element, so the tallest swimmer) height &lt;- swimmers[order(-height)][1:1] The order function, called on data.table objects calls the internal forder() function which is optimised on such objects. Counting rows is possible with different aproaches: usmedals &lt;- olympics[sport == &quot;Swimming&quot; &amp; !is.na(medal), length(medal)] or with the .N approach usmedals &lt;- olympics[sport == &quot;Swimming&quot; &amp; !is.na(medal), .N] usmedals ## [1] 3048 As you see above we come now to the second argument of data.table objects: 8.2.2 Selecting Columns Now we want to condition on the colums too. Here we can either write the column names as unquoted names inside a list. Note we can use the . as an abbreviation for list here. swimmers_gold[, list(athlete, country, medal)][1:5] # list with unquoted column names ## athlete country medal ## 1: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Great Britain Gold ## 2: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Great Britain Gold ## 3: Nathan Ghar-Jun Adrian United States Gold ## 4: Nathan Ghar-Jun Adrian United States Gold ## 5: Nathan Ghar-Jun Adrian United States Gold swimmers_gold[, .(athlete,country, medal)][1:5] # . is an abbreviation for lists ## athlete country medal ## 1: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Great Britain Gold ## 2: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Great Britain Gold ## 3: Nathan Ghar-Jun Adrian United States Gold ## 4: Nathan Ghar-Jun Adrian United States Gold ## 5: Nathan Ghar-Jun Adrian United States Gold Or we can write the quoted column names inside the c() function. swimmers[, c(&quot;athlete&quot;, &quot;medal&quot;)][1:5] # ## athlete medal ## 1: Einar Ferdinand &quot;&quot;Einari&quot;&quot; Aalto &lt;NA&gt; ## 2: Arvo Ossian Aaltonen &lt;NA&gt; ## 3: Arvo Ossian Aaltonen &lt;NA&gt; ## 4: Arvo Ossian Aaltonen Bronze ## 5: Arvo Ossian Aaltonen Bronze The tidyverse syntax for comparison was swimmers %&gt;% select(&quot;athlete&quot;, &quot;medal&quot;) This syntax also allows us to define the column in a separate step. cols &lt;- c(&quot;athlete&quot;, &quot;medal&quot;) # columns are defined in a separate step swimmers_gold[, ..cols][1:5] # note the .. syntax ## athlete medal ## 1: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Gold ## 2: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Gold ## 3: Nathan Ghar-Jun Adrian Gold ## 4: Nathan Ghar-Jun Adrian Gold ## 5: Nathan Ghar-Jun Adrian Gold or using the ‘with’ statement colums &lt;- swimmers_gold[, cols, with = FALSE] head(colums) ## athlete medal ## 1: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Gold ## 2: Rebecca &quot;&quot;Becky&quot;&quot; Adlington Gold ## 3: Nathan Ghar-Jun Adrian Gold ## 4: Nathan Ghar-Jun Adrian Gold ## 5: Nathan Ghar-Jun Adrian Gold ## 6: Nathan Ghar-Jun Adrian Gold and operations on the rows and colums at the same time: var &lt;- olympics[sport == &quot;Swimming&quot; &amp; medal == &quot;Gold&quot;, ..cols] We can also execute functions on the columns e.g. to find out the average age of all olympians olympics[, mean(age, na.rm=T)] ## [1] 25.45227 and last but not least we can directly plot within the j argument (even if the informative value is questionable here) spotify[,plot(danceability,Streams)] 8.2.3 Grouping results Now we’ll consider the third parameter of the data.table object DT[i, j, by]. Let’s count the competitions grouped by sports. (Hint: if we have a look at the data in teamsports, every athlete is listed so caution with this result.) sports &lt;- olympics[, .N, by = &quot;sport&quot;] So using all three arguments we find all swimmers, and grouping they by the country sw &lt;- olympics[sport== &quot;Swimming&quot;, .N,by = country] grouping by more colums sw &lt;- olympics[sport== &quot;Swimming&quot;, .N,by = .(country,discipline)] If we want to order the data we have to change by to keyby sw &lt;- olympics[sport== &quot;Swimming&quot;, .N,keyby = .(country,discipline)] Chaining operations on the data.table object: ordOlym &lt;- olympics[,.N, by = .(country,game)][order(-game,country)] 8.2.4 Editing Data When we want to edit a Data set of the class dataframe like df = data.frame(name = c(&quot;D&quot;,&quot;a&quot;,&quot;t&quot;,&quot;a&quot;), a = 1:4, b = 5:8, c = 9:12) df ## name a b c ## 1 D 1 5 9 ## 2 a 2 6 10 ## 3 t 3 7 11 ## 4 a 4 8 12 df$c &lt;- 13:16 df ## name a b c ## 1 D 1 5 13 ## 2 a 2 6 14 ## 3 t 3 7 15 ## 4 a 4 8 16 this is done via copying the data set and a bad performance. Datatable provides the ‘:=’ operator for an better performance. Here the Data object isn’t copied but edited by reference. Adding a column spotify[, &#39;:=&#39;(duration_s=duration_ms/1000)] this column is added ‘by reference’ which is done with a higher performance than adding a column to a data.frame object. In addition we do not have to assign the expression back to the variable. Edit an column olympics[sex==&#39;M&#39;, sex := &#39;Male&#39;] olympics[sex==&#39;F&#39;, sex := &#39;Female&#39;] Deleting an column by reference spotify[, duration_ms := NULL] 8.2.5 Side effects Sometimes we’re in the situation that we want to work with a data.table object but not on the object itself but on a copy. For example we want to add again the column duration_ms by reference, then the variable spotify_cols change too. spotify_cols &lt;- names(spotify) # Columnnames spotify_cols ## [1] &quot;Position&quot; &quot;Track.Name&quot; &quot;Artist&quot; &quot;Streams&quot; ## [5] &quot;date&quot; &quot;danceability&quot; &quot;energy&quot; &quot;loudness&quot; ## [9] &quot;speechiness&quot; &quot;acousticness&quot; &quot;instrumentalness&quot; &quot;liveness&quot; ## [13] &quot;valence&quot; &quot;tempo&quot; &quot;duration_s&quot; spotify[, &#39;:=&#39;(duration_ms=duration_s*1000)] spotify_cols ## [1] &quot;Position&quot; &quot;Track.Name&quot; &quot;Artist&quot; &quot;Streams&quot; ## [5] &quot;date&quot; &quot;danceability&quot; &quot;energy&quot; &quot;loudness&quot; ## [9] &quot;speechiness&quot; &quot;acousticness&quot; &quot;instrumentalness&quot; &quot;liveness&quot; ## [13] &quot;valence&quot; &quot;tempo&quot; &quot;duration_s&quot; &quot;duration_ms&quot; If we want to prevent these side effect we have to work on a copy. So again: spotify[, duration_ms := NULL] # deleting by reference spotify_cols ## [1] &quot;Position&quot; &quot;Track.Name&quot; &quot;Artist&quot; &quot;Streams&quot; ## [5] &quot;date&quot; &quot;danceability&quot; &quot;energy&quot; &quot;loudness&quot; ## [9] &quot;speechiness&quot; &quot;acousticness&quot; &quot;instrumentalness&quot; &quot;liveness&quot; ## [13] &quot;valence&quot; &quot;tempo&quot; &quot;duration_s&quot; spotify_cols &lt;- copy(names(spotify)) spotify[, &#39;:=&#39;(duration_ms=duration_s*1000)] spotify_cols ## [1] &quot;Position&quot; &quot;Track.Name&quot; &quot;Artist&quot; &quot;Streams&quot; ## [5] &quot;date&quot; &quot;danceability&quot; &quot;energy&quot; &quot;loudness&quot; ## [9] &quot;speechiness&quot; &quot;acousticness&quot; &quot;instrumentalness&quot; &quot;liveness&quot; ## [13] &quot;valence&quot; &quot;tempo&quot; &quot;duration_s&quot; 8.3 Runtime comparision Let us now compare the runtime of some expression made in data.table and tidyverse. Performance differs depending on the type of operation, and will likely also depend on the size of the data set. Here we test separately sorting, filtering, and grouping/aggregating for our data set of about 270.000 observations. install.packages(&quot;microbenchmark&quot;) library(microbenchmark) # package for performance comparisons olympics_tbl &lt;- as_tibble(olympics) # create a tibble to compare performance # Sorting microbenchmark( olympics_tbl %&gt;% arrange(age), # dplyr olympics[order(age)], # data.table times = 5 # run 5 time to get more robust measurements ) ## Unit: milliseconds ## expr min lq mean median uq ## olympics_tbl %&gt;% arrange(age) 96.3809 104.7948 133.48596 107.9655 126.5487 ## olympics[order(age)] 35.7042 43.1039 63.02072 43.2537 53.3242 ## max neval ## 231.7399 5 ## 139.7176 5 # Filtering microbenchmark( olympics_tbl %&gt;% filter(height &gt; 160), # dplyr olympics[height &gt; 160], # data.table times = 5 ) ## Unit: milliseconds ## expr min lq mean median uq ## olympics_tbl %&gt;% filter(height &gt; 160) 14.8139 16.1564 19.60014 16.8633 18.0434 ## olympics[height &gt; 160] 11.5326 12.5000 16.11734 13.8222 16.7287 ## max neval ## 32.1237 5 ## 26.0032 5 # Grouping microbenchmark( olympics_tbl %&gt;% group_by(athlete) %&gt;% summarise(mean(height, na.rm=T)), # dplyr olympics[, .(mean(height, na.rm=T)), keyby=athlete], # data.table times = 5 ) ## Unit: milliseconds ## expr ## olympics_tbl %&gt;% group_by(athlete) %&gt;% summarise(mean(height, na.rm = T)) ## olympics[, .(mean(height, na.rm = T)), keyby = athlete] ## min lq mean median uq max neval ## 2526.8202 2547.5901 2573.3637 2570.1466 2587.2232 2635.0385 5 ## 543.5562 549.2586 580.5558 565.2651 580.7441 663.9551 5 8.4 Further resources Here are links to further resources on the package data.table: Vignette Intro Vignette on reference semantics Github Wiki Cheatsheet data.table vs. dplyr "]
]
