# Dates and times: lubridate {#lubridate}

Resources:

- [Lubridate homepage](https://lubridate.tidyverse.org/)
- [Cheatsheet](https://rawgit.com/rstudio/cheatsheets/master/lubridate.pdf)
- [Book Chapter in R4DS](https://r4ds.had.co.nz/dates-and-times.html)
- [Vignette](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html)

Suggested data set: weather-kiel-holtenau

## What is Lubridate?
Lubridate is an R-Package designed to ease working with date/time variables. These can be quite challenging in baseR and lubridate allows for frictionless working with dates and times, hence the name.

Lubridate ist part of the tidyverse package, but can be installed seperately as well. It probably reveals most of its usefullness in collaboration with other tidyverse packages. A useful extension, depending on your data, might be the time-series package which is not part of tidyverse.

All mentioned packages can be optained with the following commands. 

package.install("lubridate")
package.install("tidyverse")

TODO @Robert in use?
package.install("time-series")

If the have been installed previously in your environment, they might have to be called upon by using 
library(tidyverse) and so forth.

## Basics

Some examples of real world date - time formats found in datasets:

How people talk about dates and times often differs from the notation of the given information. Depending on the specific use of the data, the given information might be more or less granular. When people in the USA talk distance between two places, they often give an approximation of how long it will take a person to drive from A to B and round-up or down to the hour. 

Flight schedules will most likely be exact to the minute, while some sensordata will probably need to be exact to the milisecond. So there will be differing granularity in date and time data. 

Even if this would not be a challenge, we still would have to deal with different notations of date and time. People in Germany will write a day-date like: dd.mm.yyyy or short dd.mm.yy, while the anglo-saxon realm will use mm.dd.yyyy frequently and the most chronologically sound way would be to use yyyy.mm.dd, but this doesn't seem to stick with humans. 

On top of these issues there's the fact that time itself does not make the impression of being the most exact parameter out there. Universal time might appear linear, but the way our planet revolves our galaxy has made it neccessary to adjust date and times every now and then, so our Kalender stays in tune with our defined seasons. This creates leap years, skipped seconds, daylight-savings time and last, but not least time-zones, which can mess things up even further.

Three types of date/time data
Sys.time() functions
Time Formats

## Application - Import, Clean date-time
We will apply the lubridate package to Weather data from on stationary sensor in northern Germany, the weather station in Kiel-Holtenau to be more exact.

Before we introduce the library, the prerequisites must be created.
```{r}
library(readr)          # part of the tidyverse
library(lubridate)      # the mentioned lubrication for date-time wrangling
library(tidyverse)      # the tidyverse with its dplyr functions for data wrangling
library(ggplot2)        # data visualisation package (is actually part of tidyverse, but still)
```
A first step is to import the data, which is given in a csv-format. We will use the tidyverse version of read_csv to accomplish this step.

The data will be called df (data.frame) in order to make reference in code an writing more efficient further down.

This is the simple approach to read a file with suffix csv.
```{r}
df <- read_csv("data/weather_kiel_holtenau.csv")
head(df,5)
```
All columns are recognized as double values.

By looking at the "MESS_DATUM" column it becomes apparent that this represents a timestamp that was created every 10 minutes. The standard column type definition of the import tool have´nt sufficed to format this appropriately, which is why the column will be defined as a double for now.

Using the code generator of the data import tool it is possible to format the variables in the appropriate classes. The code snippet is automatically generated by the import tool. For our concern of the timestamp variable please note the part where the column MESS_DATUM = col_datetime(format = "%Y%m%d%H%M") which will reach our present goal to format the timestamp and class as a POSIXct.

```{r}
df <- read_csv("data/weather_kiel_holtenau.csv", 
          col_types = cols(MESS_DATUM = col_datetime(format = "%Y%m%d%H%M"), 
                                                   NIEDERSCHLAGSDAUER = col_integer(), 
                                                   NIEDERSCHLAGSINDIKATOR = col_integer(), 
                                                   STATIONS_ID = col_integer()))
head(df,5)
```
We will examine the data further in a few seconds. At this point it should be mentioned againg that we generated the code to import the data by using the data import readr tool of RStudio. This tool allows a first look at the raw csv data before import and some tweaking of the variables and their classes. We left the locale portion of the import tool untouched as of now. 

The following lines of Code import the same data in the MESS_DATUM column as a string of characters again and then calls a specific lubridate function (ymd) which basically recognizes the time format in the column given only very little information. In our case we only specify the order the date-time information is given, which is Year-Month-Day-Hour-Minute. The ymd_hm function only delivers the desired result however, if it is applied to a character string, which is why we chose to overwrite the dataset df with a new import procedure that makes sure that the column MESS_DATUM is given as a character after import.
```{r}
df1 <- read_csv("data/weather_kiel_holtenau.csv", 
          col_types = cols(
            MESS_DATUM = col_character(), 
            NIEDERSCHLAGSINDIKATOR = col_integer(), 
            STATIONS_ID = col_integer()))
df1$MESS_DATUM <- ymd_hm(df1$MESS_DATUM)
head(df1,5)
```
As can be seen above, the MESS_DATUM is now in a POSIXct format again, which is what will be needed for further calculations and analysis of the data set.

Alternatively you could use the follwing parsing function to achieve the same result. Note that the parse_date_time function needs a timestamp to be in character format to work, as well.
```{r}
df2 <- read_csv("data/weather_kiel_holtenau.csv", 
    col_types = cols(MESS_DATUM = col_character()))
df2$MESS_DATUM <- parse_date_time(df2$MESS_DATUM, orders ="Ymd HM")
head(df2,5)
```
OK we have successfully formated the time-stamp data into a productive date-time format. 

## Application - Check and modify data
The next steps are a check and elimination procedure to eliminate missing values (NAs) from the dataset, 
if some observations might have failed to generate data successfully.
```{r}
df %>% 
  filter(is.na(MESS_DATUM)) %>%                   # Checking if there are observations with a missing MESS_DATUM
  view()
```
We don't encounter any NAs in the MESS_DATUM column. Eliminating NAs from the MESS_DATUM Column including the other variables of that specific observations from the data frame would have been possible with: 
```{r}
df <- df %>% 
  filter(!is.na(MESS_DATUM))                      # missing MESS_DATUM observations (NAs) would/will be excluded from the data frame
```
OK. Time to have first explorative look at the data set.
```{r}
max(df$MESS_DATUM)                                # The latest observation is dated 10 minutes before midnight on the 13th of April 2020
min(df$MESS_DATUM)                                # The earliest observaation is dated 14th of April 2019
range(df$MESS_DATUM)                              # Another way to get the same information
```

At this point we have managed to create a tidy data set and we can now proceed to explore the data further and possibly also calculate new variables or create meaningful aggregates of the data.
A look at the end of the table by the "tail" function and a glimpse at the data confirm, that we are only dealing with data from one sensor (= station), therefore we should now examine the other variables.
```{r}
range(df$STATIONS_ID)                             # This confirms: We are only dealing with data from one weather sensor in the whole data set.
range(df$TEMPERATUR)                              # We have a range of temperatues between -2.5 and +32.1 degrees  celius and contains no NULL-Value or NA
range(df$RELATIVE_FEUCHTE)                        # Delivers an unexpected range, where the minimum is -999 and the maximum is +100. Both values are either not possible (-999) or only of theoretic value (100)
range(df$NIEDERSCHLAGSDAUER)                      # This delivers a minimum of -999 and an expected maximum of 10 (minutes in a 10 minute interval).
range(df$NIEDERSCHLAGSHOEHE)                      # Again the range shows a minimum at -999 and a maximum of 7.85 which has to be interpreted as mm which equals litres per square meter (6 x 7.85 = 47.1/h = torrential)
range(df$NIEDERSCHLAGSINDIKATOR)                  # A look at the data seems to give either a 1 or a 0 for this variable, i.e. it rained or it didn't. -999 must be interpreted as a failed observation
```
We found observations in a number of variables that would need to be deleted from the data frame to get decent data.
Based on our knowledge, we have decided that the following columns are only considered if the NIEDERSCHLAGSINDIKATOR is set to 1.  This column is like a switch for the columns.
The following columns depend on the indicator value:
- NIEDERSCHLAGSDAUER
- NIEDERSCHLAGSHOEHE

This can be accomplished with the following code snippet:
```{r}
values_of_ind <- df %>% 
  group_by(NIEDERSCHLAGSINDIKATOR) %>%
  tally()
head(values_of_ind)
```
Now we modified the NIEDERSCHLAGSINDIKATOR for upcoming analyzes.
```{r}
df$NIEDERSCHLAGSINDIKATOR <- ifelse(df$NIEDERSCHLAGSINDIKATOR == -999, 0, df$NIEDERSCHLAGSINDIKATOR)
value_of_ind_n <- df %>% 
  group_by(NIEDERSCHLAGSINDIKATOR) %>%
  tally()
head(value_of_ind_n)
```


The column RELATIVE_FEUCHTE must also be adjusted.
The negative values are set to 0.
```{r}
values_of_rel <- df %>% 
  group_by(RELATIVE_FEUCHTE) %>%
  filter(RELATIVE_FEUCHTE < 0) %>%
  tally()
head(values_of_rel)
```

```{r}
df$RELATIVE_FEUCHTE <- ifelse(df$RELATIVE_FEUCHTE == -999, 0, df$RELATIVE_FEUCHTE)
values_of_rel_n <- df %>% 
  group_by(RELATIVE_FEUCHTE) %>%
  tally()
view(values_of_rel_n)
```



## Application - Before Exploration
We have clean and tidy data and begin with interpretation and exploration.

The "TEMPERATUR" is given in degrees Celsius, the "RELATIVE-FEUCHTE" is a percentage Value for humidity which refers to the degree of water saturation that is prevalent in the air at a given temperature. As temperature increases, the air can absorb larger amounts of water, hence the relativity of humidity.

The next variable is "NIEDERSCHLAGSDAUER" which is given as an integer smaller or equal to 10. Therefore it gives the time it has rained during the timestamp intervall of 10 Minutes. 
The variable "NIEDERSCHLAGSHOEHE" is a measure of rainfall intensity. Its maximum value can also be checked by the following command:
```{r}
max(df$NIEDERSCHLAGSHOEHE, na.rm = FALSE)
```
We can assume that this number gives us the amount of rainfall in milimeters, which is a common definiton and is equivalent as liters of rainfall per squaremeter in a given time intervall. A strong rainfall in central Europe can generate around 30mm/h of rainfall, i.e. 30 liters of rainfall per squaremeter per hour. The maximum value is therefore an indicator of a heavy downpour as a continuation for a whole hour would have yielded 6 x 7.85 = 47.1 litres per hour.

The next variable is simply a binary expression of rainfall (1) or no rainfall (0) in the given intervall. This is relevant to measure as some types of rainfall do not generate enough water to messure an amount of water. The dreaded Northgerman "drizzle" comes to mind.

Ágain: Since the sensor has taken a snapshot every 10 minutes, we have six observations per hour.

## Application - Exploration - Analysis
Calculate Average Temperatures (d/m/season/y) Calculate Averages for humdity, rainfall etc. Plus many more insights, that are not apparent at this draft stage.

The first variable of interest should be "TEMPERATUR". A quick visualisation delivers this picture:
```{r}
ggplot(df)+
  geom_point(aes(MESS_DATUM, TEMPERATUR), colour = "red", size = 0.1)
```
This is a representation of all observations of temperature and therefore appears quite crowded. However, a typical course of the seasons during a year can already be interpreted from this plot.

ACHTUNG - ENTWEDER IN BESSER ODER RAUS! IST NICE TO HAVE!
Versuch des Plottings einer Regression. Hier entsteht natürlich eine Regressionslinie durch alle Punkte, was keinen Sinn ergibt. 
Es müsste als eine Regression auf eine Zeitreihe definiert bzw. vermutlich auf rollierende Durchschnitte berechnet werden.
Mit einem vernünftigen rollierenden Durchschnitt kann so eine regression inkl. Standard-Error aber auch eine schöne Visualiserung abgeben (ggf. sogar ohne die Datenpunkte selbst)

```{r}
ggplot(df, aes(MESS_DATUM, TEMPERATUR))+
  geom_smooth(method = "lm", se=TRUE, color="black", formula = y ~ x) +
            geom_point(color = "red", size = 0.1)
```
Let'S try to get a clearer picture of the temperature during the course of the observed year. 
We need to form averages and aggregates to make visualisation more to the point and get less crowded picture.

New variables could accomodate another dimension of date-time data. So we could produce variables for hours, 24hdays, weeks, months and even possibly seasons.

Luckily lubridate has functions for us that will generate these new variables:

Assigning a year to every observation by creating a new column with lubridate function "year"
```{r}
df <- df %>% 
  mutate(JAHR = year(df$MESS_DATUM))
```
Assigning a month to every observation by creating a new column with lubridate function "month"
```{r}
df <- df %>% 
  mutate(MONAT = month(df$MESS_DATUM))
```
Assigning an Epiweeknr to every observation by creating a new column with lubridate function "epiweek"
ACHTUNG: Sollte der Begriff Epiweek hier noch genauer erläutert werden?
```{r}
df <- df %>% 
  mutate(EKW = epiweek(df$MESS_DATUM))
```
Assigning a daynumber to every observation by creating a new column "yday" (Year day)
```{r}
df <- df %>%
  mutate(JTAG = yday(df$MESS_DATUM))
```
Assigning an hour to every observation by creating a new column "STUNDE"
```{r}
df <- df %>%
  mutate(STUNDE = hour(df$MESS_DATUM))
```
Our data frame (df) now has 12 variables and now we can filter the data by year, month, week, day and hour of the day. This should give us possibilities to aggregate.
The following GROUP_BY command creates a unique grouping per hour and then calculates the average temperature for every hour, then pastes this value into each observation per hour.
```{r}
df_group_Stunde <- df %>%
  group_by(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE)

df_av_temp_Stunde  <- df_group_Stunde %>%
  summarise(AVGTEMPH = mean(TEMPERATUR)) %>%
  arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE)
```
This newly created table has reduced the number of observations by the factor 6 to 8,784 and reveals the average temperature per hour.
Let's plot the data again as hourly temperature averages per day:
```{r}
ggplot(df_av_temp_Stunde)+
  geom_point(aes(JTAG, AVGTEMPH), colour = "red", size = 0.1)
```
We can clearly see that the data has become aggregated and that the temperature picture becomes somewhat more clear.
Let's go on step further and shorten the data down to daily averages by:

```{r}
df_group_JTAG <- df %>%
  group_by(STATIONS_ID, JAHR, MONAT, EKW, JTAG)

df_av_temp_JTAG  <- df_group_JTAG %>%
  summarise(AVGTEMPD = mean(TEMPERATUR)) %>%
  arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG) %>%
  view()
```
This newly created table has reduced the number of observations by the factor 24 to 366 (the number of days in a leap year) and reveals the average temperature per day.
Let's plot the data again as dayly temperature averages over the whole period:
```{r}
ggplot(df_av_temp_JTAG)+
  geom_line(aes(JTAG, AVGTEMPD), colour = "red", size = 0.1)
```
This time the data starts to make more visual sense by using a line to connect the datapoints. 
The key take-aways from this plot, next to the rather trivial finding of higher summer temperatures, is the rather high volatility that appears to be attached to daily average temperature throughout the year.
Let' boil the data down to weekly temperature averages:
```{r}
df_group_EKW <- df %>%
  group_by(STATIONS_ID, JAHR, MONAT, EKW)

df_av_temp_EKW  <- df_group_EKW %>%
  summarise(AVGTEMPW = mean(TEMPERATUR)) %>%
  arrange(STATIONS_ID, JAHR, MONAT, EKW) %>%
  view()
```
and plot again:
```{r}
ggplot(df_av_temp_EKW)+
  geom_line(aes(EKW, AVGTEMPW), colour = "red", size = 0.5)
```
Let's look at monthly averages:
```{r}
df_group_MONAT <- df %>%
  group_by(STATIONS_ID, JAHR, MONAT)

df_av_temp_MONAT  <- df_group_MONAT %>%
  summarise(AVGTEMPM = mean(TEMPERATUR)) %>%
  arrange(STATIONS_ID, JAHR, MONAT) %>%
  view()
```
and plot again:
```{r}
ggplot(df_av_temp_MONAT)+
  geom_line(aes(MONAT, AVGTEMPM), colour = "red", size = 1)
```
Which is the kind of temperature curve we would expect to see in a north German location like Kiel with a clear pattern of 3 months of summer with high average temperatures.
It is interesting to note that April 2020 is apparently much warmer on average than 2019. The dimension indicates a difference of about 2 degrees celsius.

### Intervals
To check the results, it is advisable to consider small periods of time that the human eye can quickly see.

Only when the expected results have been achieved for a short period do we consider the next largest period, e.g. Week, month, quarter and year.

ACHTUNG: Hier könnte jetzt die Betrachtung von Intervallen als Anwendung auf die Jahreszeiten folgen. 
@MARCUS - Dein folgender Code würde zumindest schnell eine durchschnittsstemperatur für die jeweilige Jahreszeit liefern. Es müsste dann halt noch in einer Tabelle zusammengeführt werden.

```{r}
tag_int <- interval(ymd("2020-03-01"), ymd("2020-03-02"))

```

### Group 
The following GROUP_BY command creates a unique grouping per hour and then calculates the average temperature for every hour, then pastes this value into each observation per hour.
```{r}
df_sel <- df %>%
  filter(MESS_DATUM >= int_start(tag_int) & MESS_DATUM <= int_end(tag_int))
df_group <- df_sel %>% 
  group_by(STATIONS_ID , hour(MESS_DATUM) , day(MESS_DATUM) ) 
head(df_group, 10)

```

The columns can be renamed after a Group-By with rename function.
```{r}
df_group <- df_sel %>% 
  group_by(STATIONS_ID , hour(MESS_DATUM) , day(MESS_DATUM) ) %>% 
  rename( "STUNDE" = 'hour(MESS_DATUM)', "TAG" = 'day(MESS_DATUM)' ) 
head(df_group, 10)

```

The columns can also be renamed directly in Group-By.

```{r}
df_group <- df_sel %>% 
  group_by(STATIONS_ID , STUNDE = hour(MESS_DATUM) , TAG = day(MESS_DATUM) ) 
head(df_group, 5)

```

Now we can filter the data by year, month, week, day and hour of the day. This should give us possibilities to aggregate.

### periods and duration
The seasons align with the records.

```{r}
season_19_spring <- ymd_hm("2019-03-20 22:58", tz="CET")
print(season_19_spring)
season_19_summer <- ymd_hm("2019-06-21 17:54", tz="CET")
print(season_19_summer)
season_19_autumn <- ymd_hm("2019-09-23 09:50", tz="CET")
print(season_19_autumn)
season_19_winter <- ymd_hm("2019-12-22 05:19", tz="CET")
print(season_19_winter)
season_20_spring <- ymd_hm("2020-03-20 04:49", tz="CET")
print(season_20_spring)
# Target Duration => works fine!
season_20_spring_dur <- season_20_spring + ddays(92) + dhours(17) + dminutes(54)
print(season_20_spring_dur)
# Target Periode
season_20_spring_p <- season_20_spring + days(92) + hours(17) + minutes(54)
print(season_20_spring_p)
```
The expected date begin for summer 2020, (Sonnenwende Juni) 20. Juni 2020 23:43 MEZ, comes from the duration(season_20_spring_dur) and not the period (season_20_spring_p).
The periods has a difference of one hour to a duration.

### Average temperature for a day
In principle, no indicator has to be used to determine the average temperature, since the value is completely in the data frame.
```{r}
df_avg1 <- df_group %>% 
  summarise(avg = mean(TEMPERATUR)) %>% 
  arrange( STATIONS_ID , TAG , STUNDE) %>%
  view()  
```

### Sum with condition
The situation is different with the duration of precipitation. 
The indicator must be used here to determine the duration.

without indicator
```{r}
df_sum1 <- df_group %>% 
  summarise(sum(NIEDERSCHLAGSDAUER)) %>% 
  arrange( STATIONS_ID , TAG, STUNDE) %>%
  view()
```

with indicator and renamed column
```{r}
df_sum2 <- df_group %>% 
  summarise(MENGE = sum(NIEDERSCHLAGSDAUER[NIEDERSCHLAGSINDIKATOR==1])) %>% 
  arrange( STATIONS_ID , TAG, STUNDE) %>%
  view()
```

(Idea - check, if data is openly available as well. This could lead to an excurse about how to generate a raw dataset from this weatherstation.)



### Chapter XXx
What we can say at this point is that the temperatures in the City of Kiel have a clear seasonal pattern, but remain highly volatile intra-day and inter-day.
Weather however is not just defined as temperature, but humidity and precipitation play an integral role of our sensation and definition of weather as well.

Let's look at humidity the same way we looked at temperatures as hourly averages:

```{r}
df_av_humi_Stunde  <- df_group_Stunde %>%
  summarise(AVGHUMI = mean(RELATIVE_FEUCHTE)) %>%
  arrange(STATIONS_ID, JAHR, MONAT, EKW, JTAG, STUNDE)
```
Let's plot the data again as hourly humidity averages per day:
```{r}
ggplot(df_av_humi_Stunde)+
  geom_point(aes(JTAG, AVGHUMI), colour = "red", size = 0.1)
```
ACHTUNG: in dieser Ansicht zerstören die -999 und offenbar auch noch ein paar -125er Werte die Ansicht. Das müsste sich nach MARCUS Bereinigungsmethode ja erledigen.
Dann auch hier Tages, Wochen, Monats und Jahreszeitendurchschnitte berechnen und zeigen.

Im nächsten Schritt wären dann die Niederschläge zu zeigen. Hier haben wir es dann nicht mehr mit durchschnitten, sondern Summen (machen Stunden Sinn?) an Niederschlagsmengen bzw. ggf. auch Niederschlagsdauern pro Zeitrahmen zu tun.

Im Endergebnis haben wir vier verschiedene Variablen in unterschiedlichen Verdichtungen (Stunde, Tag, Woche, Monat und Jahreszeit)
Es könnte ggf. nochmal Sinn machen diese in zwei Paaren zu untersuchen: Niederschlagsdauer vs. Menge (die Frage/Hypothese dahinter wäre: gibt es Monate in denen es lage aber nicht heftig regnet vs. solche wo 
es nur zu kurzen aber heftigen Regengüssen kommt?) und die Frage nach der Paarung Luftfeuchtigkeit und Temperatur (feuchte vs. trockene Hitze/Kälte macht einen großen Unterschied beim subjektiven
Temperaturempfinden)

In einer finalen Visualisierung lassen sie diese Erkenntnisse ggf. nochmals prägnanter Visualisieren (eine Heatmap comes to mind).


- can we find historic KPIs to compare our findings (eg. average temperature in January in Kiel 1900)

What Visualisations make sense for our kind of data/insights? Research and try&error

## Wrap up - outlook date-time / time-series
what potential problems have not been adressed?

## Wrap up - What's next/out there?
Insights from data / data vis

What potential problems have not been adressed? (Time series package? etc?)
